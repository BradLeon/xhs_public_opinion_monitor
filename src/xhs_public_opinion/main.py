#!/usr/bin/env python
import sys
import warnings
import os
import json
import re
import pandas as pd
from typing import List
from datetime import datetime
from dotenv import load_dotenv
import logging

from xhs_public_opinion.crew import XhsPublicOpinionCrew
from xhs_public_opinion.tools import (
    DataMergerTool,
    SOVCalculatorTool,
    MultimodalBrandAnalyzer,
    BrandSentimentExtractorTool,
    SOVVisualizationTool,
    BrandSentimentVisualizationTool
)
from xhs_public_opinion.store.database import SupabaseDatabase

logger = logging.getLogger(__name__)

warnings.filterwarnings("ignore", category=SyntaxWarning, module="pysbd")

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

def _check_environment() -> bool:
    """æ£€æŸ¥å¿…è¦çš„ç¯å¢ƒå˜é‡"""
    required_env_vars = ['SEO_SUPABASE_URL', 'SEO_SUPABASE_ANON_KEY', 'DASHSCOPE_API_KEY']
    missing_vars = [var for var in required_env_vars if not os.getenv(var)]
    
    if missing_vars:
        print(f"é”™è¯¯ï¼šç¼ºå°‘å¿…è¦çš„ç¯å¢ƒå˜é‡: {', '.join(missing_vars)}")
        print("è¯·åˆ›å»º .env æ–‡ä»¶å¹¶è®¾ç½®ä»¥ä¸‹å˜é‡ï¼š")
        for var in missing_vars:
            print(f"{var}=your_{var.lower()}")
        return False
    return True

def run():
    """
    è¿è¡Œå°çº¢ä¹¦å…¬å…±èˆ†æƒ…åˆ†æï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼šå…ˆæ’åºï¼Œå†å¯¹å‰100åè¿›è¡Œå¤šæ¨¡æ€åˆ†æï¼‰
    æµç¨‹ï¼šæ•°æ®åˆå¹¶æ’åº â†’ å‰100åå¤šæ¨¡æ€åˆ†æ â†’ SOVç»Ÿè®¡ â†’ å“ç‰Œæƒ…æ„Ÿåˆ†æ â†’ å¯è§†åŒ–å›¾è¡¨ç”Ÿæˆ
    """
    try:
        # ç¯å¢ƒæ£€æŸ¥
        if not _check_environment():
            return None
        
        print("ğŸš€ å¼€å§‹æ‰§è¡Œå°çº¢ä¹¦å…¬å…±èˆ†æƒ…åˆ†æï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰...")
        print(f"ğŸ“… å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("="*60)
        
        # å¤„ç†å…³é”®è¯åˆ—è¡¨
        keywords = ["ä¸°ç›ˆè“¬æ¾æ´—å‘æ°´", "ç¼•çµ", "livingproof"]
        target_brand = "Living Proof"

        for keyword in keywords:
            print(f"\nğŸ” å¤„ç†å…³é”®è¯: {keyword}")
            print("-" * 40)
            
            
            # æ­¥éª¤1: æ•°æ®åˆå¹¶å’Œæ’åº
            print("ğŸ“Š æ­¥éª¤1: æ•°æ®åˆå¹¶å’Œæ’åº...")
            merged_data_path = _basic_data_merger(keyword=keyword)
            if not merged_data_path or "å¤±è´¥" in merged_data_path:
                print(f"   âŒ æ•°æ®åˆå¹¶å¤±è´¥: {merged_data_path}")
                continue
            
            # æ­¥éª¤2: æå–å‰100ånote_idå¹¶è¿›è¡Œå¤šæ¨¡æ€åˆ†æ
            
            print("ğŸ¤– æ­¥éª¤2: å‰100åå¤šæ¨¡æ€åˆ†æ...")
            analysis_success = _analyze_top_notes(csv_path=merged_data_path, top_n=100)
            if not analysis_success:
                print("   âš ï¸ å¤šæ¨¡æ€åˆ†æå¤±è´¥ï¼Œä½†ç»§ç»­åç»­æ­¥éª¤")
            
            # æ­¥éª¤3: SOVè®¡ç®—
            print("ğŸ“ˆ æ­¥éª¤3: SOVè®¡ç®—...")
            _sov_calculator(keyword=keyword)
            
            # æ­¥éª¤4: å“ç‰Œæƒ…æ„Ÿåˆ†æ
            print("ğŸ’ æ­¥éª¤4: å“ç‰Œæƒ…æ„Ÿåˆ†æ...")
            _extract_brand_sentiment(keyword=keyword, brand=target_brand, csv_input_path=merged_data_path)
            
            # æ­¥éª¤5: SOVå¯è§†åŒ–å›¾è¡¨ç”Ÿæˆ
            print("ğŸ“Š æ­¥éª¤5: SOVå¯è§†åŒ–å›¾è¡¨ç”Ÿæˆ...")
            _sov_visualization(keyword=keyword, target_brand=target_brand)

            # æ­¥éª¤6: å“ç‰Œæƒ…æ„Ÿåˆ†æå¯è§†åŒ–å›¾è¡¨ç”Ÿæˆ
            print("ğŸ’ æ­¥éª¤6: å“ç‰Œæƒ…æ„Ÿåˆ†æå¯è§†åŒ–å›¾è¡¨ç”Ÿæˆ...")
            _brand_sentiment_visualization(keyword=keyword, target_brand=target_brand)

            print(f"âœ… å…³é”®è¯ '{keyword}' å¤„ç†å®Œæˆ")
            
        
        print("\n" + "="*60)
        print("ğŸ‰ æ‰€æœ‰å…³é”®è¯å¤„ç†å®Œæˆ!")
        print(f"â° å®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("="*60)
        
        return True
        
    except Exception as e:
        print(f"âŒ æ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        print("è¯¦ç»†é”™è¯¯ä¿¡æ¯è¯·æŸ¥çœ‹æ—¥å¿—")
        logger.error(f"æ‰§è¡Œå¼‚å¸¸: {str(e)}", exc_info=True)
        return None

def _basic_data_merger(keyword: str) -> str:
    """æ„é€ åŸºç¡€æ•°æ®é›†"""
    print(f"   ğŸ”„ åˆå¹¶å…³é”®è¯ '{keyword}' çš„æœç´¢ç»“æœå’Œç¬”è®°è¯¦æƒ…...")
    data_merger_tool = DataMergerTool()
    res = data_merger_tool._run(keyword)
    return res

def _get_top_note_ids_from_csv(csv_path: str, top_n: int = 100) -> List[str]:
    """ä»CSVæ–‡ä»¶ä¸­æå–å‰Nåçš„note_id"""
    try:
        df = pd.read_csv(csv_path, encoding='utf-8-sig')
        
        # æŒ‰rankæ’åºï¼Œå–å‰Nå
        df_sorted = df.sort_values('rank').head(top_n)
        note_ids = df_sorted['note_id'].tolist()
        
        logger.info(f"ä»CSVæ–‡ä»¶æå–å‰{top_n}ånote_id: {len(note_ids)}ä¸ª")
        return note_ids
        
    except Exception as e:
        logger.error(f"ä»CSVæå–note_idå¤±è´¥: {e}")
        return []

def _analyze_top_notes(csv_path: str, top_n: int = 100) -> bool:
    """å¯¹å‰Nåç¬”è®°è¿›è¡Œå¤šæ¨¡æ€åˆ†æ"""
    try:
        # åˆå§‹åŒ–æ•°æ®åº“è¿æ¥
        db = SupabaseDatabase()
        if not db.is_connected():
            print("   âŒ æ•°æ®åº“è¿æ¥å¤±è´¥")
            return False
        
        # 1. ä»CSVä¸­æå–å‰Nåçš„note_id
        top_note_ids = _get_top_note_ids_from_csv(csv_path, top_n)
        if not top_note_ids:
            print("   âŒ æ— æ³•æå–å‰100ånote_id")
            return False
        
        print(f"   ğŸ“‹ æå–åˆ°å‰{top_n}åç¬”è®°ï¼Œå…±{len(top_note_ids)}ä¸ª")
        
        # 2. ç›´æ¥ä»æ•°æ®åº“è¯»å–è¿™äº›ç¬”è®°çš„è¯¦ç»†æ•°æ®
        raw_data = db.get_specific_notes_json(top_note_ids)
        
        if not raw_data or "æ²¡æœ‰æ‰¾åˆ°" in raw_data:
            print("   âŒ æ— æ³•ä»æ•°æ®åº“è¯»å–ç¬”è®°è¯¦æƒ…")
            return False
        
        # 3. è§£ææ•°æ®
        notes_data = json.loads(raw_data)
        all_notes = notes_data.get('notes', [])
        total_notes = len(all_notes)
        
        print(f"   ğŸ“– ä»æ•°æ®åº“è¯»å–åˆ°{total_notes}æ¡ç¬”è®°è¯¦æƒ…")
        
        if total_notes == 0:
            print("   âš ï¸ æ²¡æœ‰éœ€è¦åˆ†æçš„ç¬”è®°")
            return True
        
        # 4. åˆå§‹åŒ–å¤šæ¨¡æ€åˆ†æå™¨
        multimodal_analyzer = MultimodalBrandAnalyzer()
        if not multimodal_analyzer.client:
            print("   âŒ å¤šæ¨¡æ€åˆ†æå™¨åˆå§‹åŒ–å¤±è´¥ï¼Œè¯·æ£€æŸ¥DASHSCOPE_API_KEY")
            return False
        
        # 5. å¤„ç†æ¯æ¡ç¬”è®°
        success_count = 0
        failed_count = 0
        
        for i, note in enumerate(all_notes, 1):
            try:
                print(f"   ğŸ”„ åˆ†æç¬¬{i}/{total_notes}æ¡ç¬”è®° (note_id: {note.get('note_id', 'unknown')})...")
                
                # ç¡®å®šå†…å®¹ç±»å‹
                content_type = "video" if note.get('type') == 'video' and note.get('video_url') else \
                             "image" if note.get('image_list') else "text"
                
                # åˆ†æå†…å®¹
                result = multimodal_analyzer._run(json.dumps(note, ensure_ascii=False), content_type)
                parsed_result = json.loads(result)
                
                if parsed_result.get('_analysis_failed', False):
                    print(f"      âš ï¸ åˆ†æå¤±è´¥: {parsed_result.get('_error_message', 'æœªçŸ¥é”™è¯¯')}")
                    failed_count += 1
                    continue
                
                # æ·»åŠ ç¬”è®°IDå¹¶ç›´æ¥å†™å…¥æ•°æ®åº“
                parsed_result['note_id'] = note['note_id']
                write_result = db.update_single_note_analysis_json(parsed_result)
                
                # è§£æå†™å…¥ç»“æœ
                try:
                    write_response = json.loads(write_result)
                    if write_response.get('success', False):
                        success_count += 1
                        print(f"      âœ… åˆ†æå®Œæˆ")
                    else:
                        failed_count += 1
                        print(f"      âŒ å†™å…¥å¤±è´¥: {write_response.get('message', 'æœªçŸ¥é”™è¯¯')}")
                except:
                    # å¦‚æœè§£æå¤±è´¥ï¼Œæ ¹æ®è¿”å›å†…å®¹åˆ¤æ–­
                    if "âœ…" in write_result:
                        success_count += 1
                        print(f"      âœ… åˆ†æå®Œæˆ")
                    else:
                        failed_count += 1
                        print(f"      âŒ å†™å…¥å¤±è´¥: {write_result}")
                    
            except Exception as e:
                failed_count += 1
                print(f"      âŒ å¤„ç†å¼‚å¸¸: {e}")
                logger.error(f"å¤„ç†ç¬”è®°å¼‚å¸¸: {str(e)}", exc_info=True)
        
        # 6. æ‰“å°ç»Ÿè®¡ç»“æœ
        print(f"   ğŸ“Š å¤šæ¨¡æ€åˆ†æå®Œæˆ:")
        print(f"      âœ… æˆåŠŸ: {success_count} æ¡")
        print(f"      âŒ å¤±è´¥: {failed_count} æ¡")
        print(f"      ğŸ“ˆ æˆåŠŸç‡: {(success_count/total_notes*100):.1f}%")
        
        return success_count > 0
        
    except Exception as e:
        print(f"   âŒ å¤šæ¨¡æ€åˆ†æå¼‚å¸¸: {e}")
        logger.error(f"å¤šæ¨¡æ€åˆ†æå¼‚å¸¸: {str(e)}", exc_info=True)
        return False

def _sov_calculator(keyword: str) -> bool:
    """è®¡ç®—SOV"""
    print(f"   ğŸ“Š è®¡ç®—å…³é”®è¯ '{keyword}' çš„SOV...")
    sov_calculator_tool = SOVCalculatorTool()
    res = sov_calculator_tool._run(keyword, method="simple")
    
    if "è®¡ç®—å¤±è´¥" not in res:
        print("   âœ… SOVè®¡ç®—å®Œæˆ")
        return True
    else:
        print(f"   âŒ SOVè®¡ç®—å¤±è´¥: {res}")
        return False

def _extract_brand_sentiment(keyword: str, brand: str = "", csv_input_path: str = "") -> bool:
    """æå–å“ç‰Œæƒ…æ„Ÿåˆ†æç»“æœ"""
    brand_name = brand or "æ‰€æœ‰å“ç‰Œ"
    print(f"   ğŸ’ æå–å“ç‰Œæƒ…æ„Ÿåˆ†æ - {brand_name}...")
    
    brand_sentiment_extractor = BrandSentimentExtractorTool()
    res = brand_sentiment_extractor._run(keyword=keyword, brand=brand, csv_input_path=csv_input_path)
    
    if "å¤„ç†å¤±è´¥" not in res:
        print("   âœ… å“ç‰Œæƒ…æ„Ÿåˆ†æå®Œæˆ")
        return True
    else:
        print(f"   âŒ å“ç‰Œæƒ…æ„Ÿåˆ†æå¤±è´¥: {res}")
        return False

def _sov_visualization(keyword: str, target_brand: str = "") -> bool:
    """ç”ŸæˆSOVå¯è§†åŒ–å›¾è¡¨"""
    target_info = f" (ç›®æ ‡å“ç‰Œ: {target_brand})" if target_brand else ""
    print(f"   ğŸ“Š ç”ŸæˆSOVå¯è§†åŒ–å›¾è¡¨{target_info}...")
    
    sov_visualization_tool = SOVVisualizationTool()
    res = sov_visualization_tool._run(keyword=keyword, target_brand=target_brand)
    
    if "âœ…" in res:
        print("   âœ… SOVå›¾è¡¨ç”Ÿæˆå®Œæˆ")
        return True
    else:
        print(f"   âŒ SOVå›¾è¡¨ç”Ÿæˆå¤±è´¥: {res}")
        return False

def _brand_sentiment_visualization(keyword: str, target_brand: str) -> bool:
    """ç”Ÿæˆå“ç‰Œæƒ…æ„Ÿåˆ†æå¯è§†åŒ–å›¾è¡¨"""
    print(f"   ğŸ’ ç”Ÿæˆå“ç‰Œæƒ…æ„Ÿåˆ†æå›¾è¡¨ - {target_brand}...")
    
    brand_sentiment_visualization_tool = BrandSentimentVisualizationTool()
    res = brand_sentiment_visualization_tool._run(keyword=keyword, target_brand=target_brand)
    
    if "âœ…" in res:
        print("   âœ… å“ç‰Œæƒ…æ„Ÿåˆ†æå›¾è¡¨ç”Ÿæˆå®Œæˆ")
        return True
    else:
        print(f"   âŒ å“ç‰Œæƒ…æ„Ÿåˆ†æå›¾è¡¨ç”Ÿæˆå¤±è´¥: {res}")
        return False

def train():
    """è®­ç»ƒcrewæ¨¡å‹"""
    if len(sys.argv) < 3:
        print("ä½¿ç”¨æ–¹æ³•: python -m xhs_public_opinion.main train <è¿­ä»£æ¬¡æ•°> <è®­ç»ƒæ–‡ä»¶å>")
        return None
        
    try:
        print(f"ğŸ¯ å¼€å§‹è®­ç»ƒæ¨¡å‹ï¼Œè¿­ä»£æ¬¡æ•°: {sys.argv[1]}")
        result = XhsPublicOpinionCrew().crew().train(
            n_iterations=int(sys.argv[1]), 
            filename=sys.argv[2], 
            inputs={"analysis_type": "å°çº¢ä¹¦ç¬”è®°æƒ…æ„Ÿåˆ†æè®­ç»ƒ"}
        )
        print("âœ… è®­ç»ƒå®Œæˆ!")
        return result

    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        print("è¯¦ç»†é”™è¯¯ä¿¡æ¯è¯·æŸ¥çœ‹æ—¥å¿—")
        return None

def replay():
    """é‡æ”¾crewæ‰§è¡Œè¿‡ç¨‹"""
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•: python -m xhs_public_opinion.main replay <ä»»åŠ¡ID>")
        return None
        
    try:
        print(f"ğŸ”„ å¼€å§‹é‡æ”¾ä»»åŠ¡: {sys.argv[1]}")
        result = XhsPublicOpinionCrew().crew().replay(task_id=sys.argv[1])
        print("âœ… é‡æ”¾å®Œæˆ!")
        return result

    except Exception as e:
        print(f"âŒ é‡æ”¾è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        print("è¯¦ç»†é”™è¯¯ä¿¡æ¯è¯·æŸ¥çœ‹æ—¥å¿—")
        return None

def test():
    """æµ‹è¯•crewåŠŸèƒ½"""
    try:
        print("ğŸ§ª å¼€å§‹æµ‹è¯•crewåŠŸèƒ½...")
        result = XhsPublicOpinionCrew().crew().test(
            n_iterations=1,
            openai_model_name="gpt-4o-mini"
        )
        print("âœ… æµ‹è¯•å®Œæˆ!")
        return result

    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        print("è¯¦ç»†é”™è¯¯ä¿¡æ¯è¯·æŸ¥çœ‹æ—¥å¿—")
        return None

if __name__ == "__main__":
    if len(sys.argv) > 1:
        command = sys.argv[1]
        if command == "train":
            train()
        elif command == "replay": 
            replay()
        elif command == "test":
            test()
        else:
            print("æœªçŸ¥å‘½ä»¤ï¼Œä½¿ç”¨ run() æ‰§è¡Œé»˜è®¤åˆ†æ")
            run()
    else:
        run()
