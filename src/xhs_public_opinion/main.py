#!/usr/bin/env python
import sys
import warnings
import os
import json
import re
from datetime import datetime
from dotenv import load_dotenv
import logging


from xhs_public_opinion.crew import XhsPublicOpinionCrew
from xhs_public_opinion.tools import (
    DatabaseReaderTool,
    DatabaseWriterTool,
    DataMergerTool,
    SOVCalculatorTool
)
from xhs_public_opinion.config.batch_config import BatchConfig

logger = logging.getLogger(__name__)

warnings.filterwarnings("ignore", category=SyntaxWarning, module="pysbd")

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# This main file is intended to be a way for you to run your
# crew locally, so refrain from adding unnecessary logic into this file.
# Replace with inputs you want to test with, it will automatically
# interpolate any tasks and agents information

def run():
    """
    è¿è¡Œå°çº¢ä¹¦å…¬å…±èˆ†æƒ…åˆ†æï¼ˆé‡æ„ç‰ˆæœ¬ï¼šæ•°æ®åº“æ‰¹é‡æ“ä½œï¼ŒAIåˆ†æ‰¹å¤„ç†ï¼‰
    æµç¨‹ï¼šå¤§æ‰¹é‡æ•°æ®åº“è¯»å– â†’ åˆ†æ‰¹AIå†…å®¹åˆ†æ â†’ æ‰¹é‡æ•°æ®åº“å†™å…¥
    """
    # ç¯å¢ƒæ£€æŸ¥
    if not _check_environment():
        return None
    
    # åˆå§‹åŒ–é…ç½®
    db_batch_size, ai_batch_size = BatchConfig.validate_and_adjust()
    current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    
    try:
        _print_startup_info(current_time)
        
        # é˜¶æ®µ1ï¼šæ‰¹é‡æ•°æ®åº“è¯»å–
        '''
        notes_data = _read_database_batch(db_batch_size)
        if not notes_data:
            return _create_empty_result("no_data", "æ²¡æœ‰æ‰¾åˆ°æœªå¤„ç†çš„ç¬”è®°æ•°æ®")
        
        total_notes_count = notes_data['total_count']
        all_notes_list = notes_data['notes']
        
        # é˜¶æ®µ2ï¼šåˆ†æ‰¹AIåˆ†æå’Œå†™å…¥
        stats = _process_all_batches(all_notes_list, ai_batch_size, current_time)
        
        # ç»“æœç»Ÿè®¡å’Œæ€»ç»“
        _print_final_statistics(stats, total_notes_count, ai_batch_size)
        '''

        # é˜¶æ®µ3ï¼šæ±‡æ€»æœç´¢ç»“æœåº•è¡¨
        _basic_data_merger(keyword="ä¸°ç›ˆè“¬æ¾æ´—å‘æ°´")
        # é˜¶æ®µ4ï¼šè®¡ç®—SOV
        _sov_calculator(keyword="ä¸°ç›ˆè“¬æ¾æ´—å‘æ°´")

        return True
  
        
    except Exception as e:
        print(f"âŒ æ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        print("è¯¦ç»†é”™è¯¯ä¿¡æ¯è¯·æŸ¥çœ‹æ—¥å¿—")
        return None


def _check_environment() -> bool:
    """æ£€æŸ¥å¿…è¦çš„ç¯å¢ƒå˜é‡"""
    required_env_vars = ['SEO_SUPABASE_URL', 'SEO_SUPABASE_ANON_KEY', 'OPENROUTER_API_KEY']
    missing_vars = [var for var in required_env_vars if not os.getenv(var)]
    
    if missing_vars:
        print(f"é”™è¯¯ï¼šç¼ºå°‘å¿…è¦çš„ç¯å¢ƒå˜é‡: {', '.join(missing_vars)}")
        print("è¯·åˆ›å»º .env æ–‡ä»¶å¹¶è®¾ç½®ä»¥ä¸‹å˜é‡ï¼š")
        print("SEO_SUPABASE_URL=your_supabase_url")
        print("SEO_SUPABASE_ANON_KEY=your_supabase_anon_key") 
        print("OPENROUTER_API_KEY=your_openrouter_api_key")
        return False
    return True


def _print_startup_info(current_time: str):
    """æ‰“å°å¯åŠ¨ä¿¡æ¯"""
    print("ğŸš€ å¼€å§‹æ‰§è¡Œå°çº¢ä¹¦å…¬å…±èˆ†æƒ…åˆ†æï¼ˆæ‰¹é‡æ•°æ®åº“+åˆ†æ‰¹AIå¤„ç†ï¼‰...")
    print(f"ğŸ“… åˆ†ææ—¶é—´: {current_time}")
    print(BatchConfig.get_config_summary())
    print("="*60)


def _read_database_batch(db_batch_size: int) -> dict:
    """è¯»å–æ•°æ®åº“æ‰¹æ¬¡æ•°æ®"""
    print("ğŸ“– é˜¶æ®µ1: æ‰¹é‡æ•°æ®åº“è¯»å–...")
    db_reader = DatabaseReaderTool()
    raw_data = db_reader._run(batch_size=db_batch_size)
    
    if not raw_data or "æ²¡æœ‰æ‰¾åˆ°æœªå¤„ç†çš„ç¬”è®°æ•°æ®" in raw_data:
        print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°æœªå¤„ç†çš„æ•°æ®ï¼Œåˆ†æç»“æŸ")
        return None
    
    try:
        notes_data = json.loads(raw_data)
        total_notes_count = notes_data.get('total_count', 0)
        
        if total_notes_count == 0:
            print("âš ï¸ è¯»å–åˆ°çš„æ•°æ®ä¸ºç©ºï¼Œåˆ†æç»“æŸ")
            return None
        
        print(f"âœ… æ•°æ®è¯»å–å®Œæˆ! å…±è·å– {total_notes_count} æ¡ç¬”è®°æ•°æ®")
        return notes_data
        
    except json.JSONDecodeError as e:
        print(f"âŒ æ•°æ®è§£æå¤±è´¥: {e}")
        return None


def _process_all_batches(all_notes_list: list, ai_batch_size: int, current_time: str) -> dict:
    """å¤„ç†æ‰€æœ‰æ‰¹æ¬¡çš„AIåˆ†æå’Œæ•°æ®åº“å†™å…¥"""
    print("\n" + "="*60)
    print("ğŸ¤– é˜¶æ®µ2: åˆ†æ‰¹æ‰§è¡ŒAIå†…å®¹åˆ†æ + å³æ—¶æ•°æ®åº“å†™å…¥...")
    
    stats = {
        'total_analysis_results': 0,
        'total_written_results': 0,
        'failed_batches': [],
        'successful_batches': []
    }
    
    db_writer = DatabaseWriterTool()
    total_notes_count = len(all_notes_list)
    
    for batch_index in range(0, total_notes_count, ai_batch_size):
        batch_end = min(batch_index + ai_batch_size, total_notes_count)
        current_batch = all_notes_list[batch_index:batch_end]
        batch_number = (batch_index // ai_batch_size) + 1
        total_batches = (total_notes_count + ai_batch_size - 1) // ai_batch_size
        
        print(f"\nğŸ”„ å¤„ç†ç¬¬ {batch_number}/{total_batches} æ‰¹ (ç´¢å¼• {batch_index}~{batch_end-1}, å…± {len(current_batch)} æ¡)")
        
        # å¤„ç†å•ä¸ªæ‰¹æ¬¡
        batch_result = _process_single_batch(current_batch, batch_number, current_time, db_writer)
        
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        if batch_result['success']:
            stats['successful_batches'].append(batch_result['stats'])
            stats['total_analysis_results'] += batch_result['stats']['analyzed']
            stats['total_written_results'] += batch_result['stats']['written']
        else:
            stats['failed_batches'].append(batch_result['error_info'])
    
    return stats


def _process_single_batch(current_batch: list, batch_number: int, current_time: str, db_writer) -> dict:
    """å¤„ç†å•ä¸ªæ‰¹æ¬¡"""
    try:
        # AIåˆ†æé˜¶æ®µ
        crew_result = _run_ai_analysis(current_batch, batch_number, current_time)
        if not crew_result:
            return _create_batch_error(batch_number, 'AIåˆ†ææ— è¿”å›ç»“æœ', len(current_batch))
        
        # è§£æåˆ†æç»“æœ
        parse_result = _parse_batch_results(crew_result, batch_number)
        if not parse_result['success']:
            return _create_batch_error(batch_number, parse_result['error'], len(current_batch))
        
        successful_results = parse_result['successful_results']
        failed_results = parse_result['failed_results']
        
        # æ•°æ®åº“å†™å…¥
        if successful_results:
            written_count = _write_results_to_database(successful_results, batch_number, db_writer)
            return _create_batch_success(batch_number, len(parse_result['all_results']), 
                                       len(successful_results), len(failed_results), written_count)
        else:
            return _create_batch_error(batch_number, 'æ‰¹æ¬¡å†…æ‰€æœ‰ç»“æœéƒ½è§£æå¤±è´¥', len(current_batch),
                                     len(successful_results), len(failed_results))
            
    except Exception as e:
        return _create_batch_error(batch_number, str(e), len(current_batch))


def _run_ai_analysis(current_batch: list, batch_number: int, current_time: str):
    """æ‰§è¡ŒAIåˆ†æ"""
    crew_inputs = {
        'analysis_type': f'å°çº¢ä¹¦ç¬”è®°æƒ…æ„Ÿåˆ†æ - ç¬¬{batch_number}æ‰¹',
        'current_date': current_time,
        'batch_size': len(current_batch),
        'notes': current_batch
    }
    
    print(f"ğŸ“‹ å¼€å§‹AIåˆ†æ: {len(current_batch)} æ¡ç¬”è®°")
    return XhsPublicOpinionCrew().crew().kickoff(inputs=crew_inputs)


def _parse_batch_results(crew_result, batch_number: int) -> dict:
    """è§£ææ‰¹æ¬¡ç»“æœ"""
    try:
        analysis_json = str(crew_result)
        print(f"âœ… ç¬¬{batch_number}æ‰¹AIåˆ†æå®Œæˆ! ç»“æœé•¿åº¦: {len(analysis_json)} å­—ç¬¦")
        
        # è§£æJSON
        if isinstance(crew_result, str):
            batch_results = json.loads(analysis_json)
        else:
            batch_results = json.loads(str(crew_result))
        
        if not isinstance(batch_results, list):
            return {'success': False, 'error': 'ç»“æœæ ¼å¼ä¸æ­£ç¡®ï¼ˆéæ•°ç»„ï¼‰'}
        
        # åˆ†ç¦»æˆåŠŸå’Œå¤±è´¥çš„ç»“æœ
        successful_results = []
        failed_results = []
        
        for i, result in enumerate(batch_results):
            if isinstance(result, dict) and result.get('_analysis_failed', False):
                failed_results.append({
                    'index': i,
                    'error_type': result.get('_error_type', 'unknown'),
                    'error_message': result.get('_error_message', 'unknown error')
                })
                logger.warning(f"[Main] ç¬¬{batch_number}æ‰¹ç¬¬{i+1}æ¡ç»“æœåˆ†æå¤±è´¥: {result.get('_error_message', 'unknown')}")
            else:
                # ç§»é™¤å¤±è´¥æ ‡è®°å­—æ®µ
                cleaned_result = {k: v for k, v in result.items() if not k.startswith('_analysis_')}
                successful_results.append(cleaned_result)
        
        print(f"ğŸ“Š ç¬¬{batch_number}æ‰¹è§£æå®Œæˆ: æ€»è®¡ {len(batch_results)} æ¡ï¼ŒæˆåŠŸ {len(successful_results)} æ¡ï¼Œå¤±è´¥ {len(failed_results)} æ¡")
        
        if failed_results:
            _print_failure_details(batch_number, failed_results)
        
        return {
            'success': True,
            'all_results': batch_results,
            'successful_results': successful_results,
            'failed_results': failed_results
        }
        
    except json.JSONDecodeError as e:
        error_msg = f'JSONè§£æå¤±è´¥: {e}'
        print(f"âš ï¸ ç¬¬{batch_number}æ‰¹ç»“æœè§£æå¤±è´¥: {e}")
        return {'success': False, 'error': error_msg}


def _print_failure_details(batch_number: int, failed_results: list):
    """æ‰“å°å¤±è´¥è¯¦æƒ…"""
    print(f"âš ï¸ ç¬¬{batch_number}æ‰¹å¤±è´¥è¯¦æƒ…:")
    for fail_info in failed_results[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ªå¤±è´¥ä¿¡æ¯
        print(f"   - ç¬¬{fail_info['index']+1}æ¡: {fail_info['error_type']} - {fail_info['error_message'][:50]}...")
    if len(failed_results) > 3:
        print(f"   - è¿˜æœ‰ {len(failed_results)-3} æ¡å¤±è´¥...")


def _write_results_to_database(successful_results: list, batch_number: int, db_writer) -> int:
    """å†™å…¥ç»“æœåˆ°æ•°æ®åº“"""
    print(f"ğŸ’¾ ç«‹å³å†™å…¥ç¬¬{batch_number}æ‰¹æˆåŠŸç»“æœåˆ°æ•°æ®åº“ï¼ˆ{len(successful_results)} æ¡ï¼‰...")
    
    success_batch_json = json.dumps(successful_results, ensure_ascii=False)
    write_result = db_writer._run(success_batch_json)
    
    if "æˆåŠŸå†™å…¥" in write_result or "âœ…" in write_result:
        # ä»å†™å…¥ç»“æœä¸­æå–æˆåŠŸæ•°é‡
        success_match = re.search(r'æˆåŠŸå†™å…¥[ï¼š:]\s*(\d+)', write_result)
        written_count = int(success_match.group(1)) if success_match else len(successful_results)
        print(f"âœ… ç¬¬{batch_number}æ‰¹æ•°æ®åº“å†™å…¥å®Œæˆ: {written_count} æ¡")
        return written_count
    else:
        print(f"âŒ ç¬¬{batch_number}æ‰¹æ•°æ®åº“å†™å…¥å¤±è´¥")
        return 0


def _create_batch_success(batch_number: int, analyzed: int, successful: int, failed: int, written: int) -> dict:
    """åˆ›å»ºæ‰¹æ¬¡æˆåŠŸç»“æœ"""
    print(f"ğŸ“Š ç¬¬{batch_number}æ‰¹å¤„ç†ç»“æœ: AIåˆ†æ {analyzed} æ¡ â†’ æˆåŠŸ {successful} æ¡ â†’ æ•°æ®åº“å†™å…¥ {written} æ¡")
    return {
        'success': True,
        'stats': {
            'batch': batch_number,
            'analyzed': analyzed,
            'successful_analyzed': successful,
            'failed_analyzed': failed,
            'written': written
        }
    }


def _create_batch_error(batch_number: int, error: str, note_count: int, success_count: int = 0, fail_count: int = 0) -> dict:
    """åˆ›å»ºæ‰¹æ¬¡é”™è¯¯ç»“æœ"""
    print(f"âŒ ç¬¬{batch_number}æ‰¹å¤„ç†å¤±è´¥: {error}")
    return {
        'success': False,
        'error_info': {
            'batch': batch_number,
            'error': error,
            'note_count': note_count,
            'success_count': success_count,
            'fail_count': fail_count
        }
    }


def _print_final_statistics(stats: dict, total_notes_count: int, ai_batch_size: int):
    """æ‰“å°æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯"""
    print(f"\nğŸ“Š åˆ†æ‰¹å¤„ç†å®Œæˆç»Ÿè®¡:")
    print(f"âœ… æˆåŠŸæ‰¹æ¬¡: {len(stats['successful_batches'])}")
    print(f"âŒ å¤±è´¥æ‰¹æ¬¡: {len(stats['failed_batches'])}")
    print(f"ğŸ“ˆ æ€»AIåˆ†æç»“æœ: {stats['total_analysis_results']} æ¡")
    print(f"ğŸ’¾ æ€»æ•°æ®åº“å†™å…¥: {stats['total_written_results']} æ¡")
    
    print("\n" + "="*60)
    print("ğŸ“‹ æ‰§è¡Œæ€»ç»“:")
    print(f"âœ… æ•°æ®åº“è¯»å–é˜¶æ®µ: å®Œæˆ (è¯»å– {total_notes_count} æ¡è®°å½•)")
    print(f"âœ… AIå†…å®¹åˆ†æé˜¶æ®µ: å®Œæˆ (åˆ† {(total_notes_count + ai_batch_size - 1) // ai_batch_size} æ‰¹å¤„ç†)")
    print(f"âœ… æ•°æ®åº“å†™å…¥é˜¶æ®µ: å®Œæˆ (å†™å…¥ {stats['total_written_results']} æ¡åˆ†æç»“æœ)") 
    print(f"â° æ€»æ‰§è¡Œæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*60)


def _create_empty_result(status: str, message: str) -> dict:
    """åˆ›å»ºç©ºç»“æœ"""
    return {
        "status": status,
        "message": message,
        "timestamp": datetime.now().isoformat()
    }


def _create_success_result(total_notes_count: int, ai_batch_size: int, stats: dict) -> dict:
    """åˆ›å»ºæˆåŠŸç»“æœ"""
    return {
        "status": "success",
        "data_read_count": total_notes_count,
        "analysis_batches": (total_notes_count + ai_batch_size - 1) // ai_batch_size,
        "analysis_results_count": stats['total_analysis_results'],
        "final_results": {
            "successful_batches": stats['successful_batches'],
            "failed_batches": stats['failed_batches']
        },
        "timestamp": datetime.now().isoformat()
    }

def _basic_data_merger(keyword: str) -> bool:
    """æ„é€ åŸºç¡€æ•°æ®é›†"""
    data_merger_tool = DataMergerTool()
    res = data_merger_tool._run(keyword)
    if "æ•°æ®æ‹¼æ¥å¤±è´¥" in res:
        return False
    return True

def _sov_calculator(keyword: str) -> bool:
    """è®¡ç®—SOV"""
    print("ğŸ“– é˜¶æ®µ2: è®¡ç®—SOV...")
    sov_calculator_tool = SOVCalculatorTool()
    res = sov_calculator_tool._run(keyword, method="simple")
    if "è®¡ç®—å¤±è´¥" in res:
        return False
    return True

def train():
    """
    è®­ç»ƒcrewæ¨¡å‹
    """
    if len(sys.argv) < 3:
        print("ä½¿ç”¨æ–¹æ³•: python -m xhs_public_opinion.main train <è¿­ä»£æ¬¡æ•°> <è®­ç»ƒæ–‡ä»¶å>")
        return None
        
    inputs = {
        "analysis_type": "å°çº¢ä¹¦ç¬”è®°æƒ…æ„Ÿåˆ†æè®­ç»ƒ"
    }
    try:
        print(f"ğŸ¯ å¼€å§‹è®­ç»ƒæ¨¡å‹ï¼Œè¿­ä»£æ¬¡æ•°: {sys.argv[1]}")
        result = XhsPublicOpinionCrew().crew().train(
            n_iterations=int(sys.argv[1]), 
            filename=sys.argv[2], 
            inputs=inputs
        )
        print("âœ… è®­ç»ƒå®Œæˆ!")
        return result

    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        print("è¯¦ç»†é”™è¯¯ä¿¡æ¯è¯·æŸ¥çœ‹æ—¥å¿—")
        return None

def replay():
    """
    é‡æ”¾crewæ‰§è¡Œè¿‡ç¨‹
    """
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•: python -m xhs_public_opinion.main replay <ä»»åŠ¡ID>")
        return None
        
    try:
        print(f"ğŸ”„ å¼€å§‹é‡æ”¾ä»»åŠ¡: {sys.argv[1]}")
        result = XhsPublicOpinionCrew().crew().replay(task_id=sys.argv[1])
        print("âœ… é‡æ”¾å®Œæˆ!")
        return result

    except Exception as e:
        print(f"âŒ é‡æ”¾è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        print("è¯¦ç»†é”™è¯¯ä¿¡æ¯è¯·æŸ¥çœ‹æ—¥å¿—")
        return None

def test():
    """
    æµ‹è¯•crewåŠŸèƒ½
    """
    try:
        print("ğŸ§ª å¼€å§‹æµ‹è¯•crewåŠŸèƒ½...")
        result = XhsPublicOpinionCrew().crew().test(
            n_iterations=1,
            openai_model_name="gpt-4o-mini"
        )
        print("âœ… æµ‹è¯•å®Œæˆ!")
        return result

    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        print("è¯¦ç»†é”™è¯¯ä¿¡æ¯è¯·æŸ¥çœ‹æ—¥å¿—")
        return None

if __name__ == "__main__":
    if len(sys.argv) > 1:
        command = sys.argv[1]
        if command == "train":
            train()
        elif command == "replay": 
            replay()
        elif command == "test":
            test()
        else:
            print("æœªçŸ¥å‘½ä»¤ï¼Œä½¿ç”¨ run() æ‰§è¡Œé»˜è®¤åˆ†æ")
            run()
    else:
        run()
