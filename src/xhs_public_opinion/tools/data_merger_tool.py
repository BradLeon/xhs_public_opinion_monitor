import os
import json
import pandas as pd
from typing import List, Dict, Any, Optional
from crewai.tools import BaseTool
import logging
from datetime import datetime
from pydantic import BaseModel, Field

from .brand_normalizer import BrandNormalizer, get_brand_normalizer
from ..store import SupabaseDatabase, FileManager

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DataMergerTool:
    """æ•°æ®æ‹¼æ¥å·¥å…· - å°†æœç´¢ç»“æœè¡¨å’Œç¬”è®°è¯¦æƒ…è¡¨è¿æ¥ç”Ÿæˆå®½è¡¨"""
    name: str = "data_merger"
    description: str = "æ ¹æ®æŒ‡å®šå…³é”®è¯ï¼Œå°†xhs_search_resultå’Œxhs_noteè¡¨æ ¹æ®note_idè¿æ¥ï¼Œç”ŸæˆCSVå®½è¡¨æ–‡ä»¶ï¼Œä½¿ç”¨RRFç®—æ³•åˆå¹¶å¤šè´¦æˆ·æ’å"
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        
        # åˆå§‹åŒ–å“ç‰Œæ ‡å‡†åŒ–å™¨
        self.brand_normalizer = get_brand_normalizer()
        
        # åˆå§‹åŒ–æ•°æ®åº“è¿æ¥
        self.db = SupabaseDatabase()
        self.file_manager = FileManager()
        
        # ä¸­è‹±æ–‡åˆ—åæ˜ å°„
        self.column_mapping = {
        "keyword": "æœç´¢å…³é”®è¯",
        "rank": "æœç´¢æ’å",
        "note_id": "ç¬”è®°ID",
        "type": "ç¬”è®°ç±»å‹",
        "title": "ç¬”è®°æ ‡é¢˜",
        "desc": "ç¬”è®°æè¿°",
        "note_url": "ç¬”è®°URL",
        "author_id": "ä½œè€…ID",
        "nickname": "ä½œè€…æ˜µç§°",
        "last_update_time": "ç¬”è®°æœ€åæ›´æ–°æ—¶é—´",
        "liked_count": "ç‚¹èµæ•°",
        "collected_count": "æ”¶è—æ•°",
        "comment_count": "è¯„è®ºæ•°",
        "share_count": "åˆ†äº«æ•°",
        "brand_list": "å“ç‰Œåˆ—è¡¨",
        "spu_list": "SPUåˆ—è¡¨",
        "emotion_dict": "æƒ…æ„Ÿå€¾å‘",
        "evaluation_dict": "é«˜é¢‘è¯",
        "data_crawler_time": "æ•°æ®é‡‡é›†æ—¶é—´",
    }
        
        logger.info(f"[DataMergerTool] ä½¿ç”¨RRFå€’æ•°æ’åèåˆç®—æ³•è¿›è¡Œå¤šè´¦æˆ·æ’ååˆå¹¶")
    
    def _run(self, keyword: str, output_dir_inner: str = "data/export",  output_dir_outer: str = "outputs") -> str:
        """
        æ‰§è¡Œæ•°æ®æ‹¼æ¥ä»»åŠ¡
        
        Args:
            keyword: è¦æŸ¥è¯¢çš„å…³é”®è¯
            output_dir: è¾“å‡ºç›®å½•ï¼Œé»˜è®¤ä¸ºdata/export
            
        Returns:
            è¿”å›æ“ä½œç»“æœä¿¡æ¯
        """
        try:
            logger.info(f"[DataMergerTool] å¼€å§‹ä¸ºå…³é”®è¯ '{keyword}' è¿›è¡Œæ•°æ®æ‹¼æ¥")
            
            # 1. ä»xhs_search_resultè¡¨è·å–æŒ‡å®šå…³é”®è¯çš„æœç´¢ç»“æœ
            search_results = self._get_search_results(keyword)
            if not search_results:
                return f"å…³é”®è¯ '{keyword}' æ²¡æœ‰æ‰¾åˆ°æœç´¢ç»“æœæ•°æ®"
            
            logger.info(f"[DataMergerTool] è·å–åˆ° {len(search_results)} æ¡æœç´¢ç»“æœ")
            
            # 2. åˆå¹¶å¤šè´¦æˆ·æ’åºç»“æœ
            merged_rankings = self._merge_multi_account_rankings(search_results)
            logger.info(f"[DataMergerTool] åˆå¹¶åå¾—åˆ° {len(merged_rankings)} ä¸ªå”¯ä¸€note_idçš„æ’åº")
            
            # 3. æå–æ‰€æœ‰note_id
            note_ids = list(merged_rankings.keys())
            
            logger.info(f"[DataMergerTool] æå–åˆ° {len(note_ids)} ä¸ªå”¯ä¸€çš„note_id")
            
            # 4. ä»xhs_noteè¡¨è·å–å¯¹åº”çš„ç¬”è®°è¯¦æƒ…
            note_details = self._get_note_details(note_ids)
            
            logger.info(f"[DataMergerTool] è·å–åˆ° {len(note_details)} æ¡ç¬”è®°è¯¦æƒ…")
            
            # 5. æ•°æ®æ‹¼æ¥ï¼ˆä½¿ç”¨åˆå¹¶åçš„æ’åºç»“æœï¼‰
            merged_data = self._merge_data_with_rankings(merged_rankings, note_details, keyword)
            
            # 6. ç”ŸæˆCSVæ–‡ä»¶ï¼ˆåªä¿å­˜å‰100åï¼‰
            csv_path = self._save_to_csv(merged_data, keyword, output_dir_inner, output_dir_outer)
            
            # 7. ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Šï¼ˆåŸºäºå‰100åæ•°æ®ï¼‰
            # æŒ‰rankæ’åºå¹¶åªå–å‰100åç”¨äºç»Ÿè®¡
            sorted_data = sorted(merged_data, key=lambda x: x.get('rank', float('inf')))
            top_100_data = sorted_data[:100]
            stats = self._generate_statistics(top_100_data, keyword)
            logger.info(f"""âœ… æ•°æ®æ‹¼æ¥å®Œæˆï¼

ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:
- å…³é”®è¯: {keyword}
- åŸå§‹æœç´¢ç»“æœè®°å½•: {len(search_results)} æ¡
- åˆå¹¶åå”¯ä¸€ç¬”è®°æ•°: {len(note_ids)} ä¸ª
- ç­›é€‰å‰100åè®°å½•: {len(top_100_data)} æ¡
- æˆåŠŸåŒ¹é…: {stats['matched_count']} æ¡
- æœªåŒ¹é…: {stats['unmatched_count']} æ¡
- è¾“å‡ºæ–‡ä»¶: {csv_path}

ğŸ“ˆ æ•°æ®æ¦‚è§ˆï¼ˆå‰100åï¼‰:
- æ€»è®°å½•æ•°: {len(top_100_data)}
- åŒ…å«å“ç‰Œä¿¡æ¯çš„è®°å½•: {stats['with_brand_count']} æ¡
- æ¶‰åŠå“ç‰Œæ•°: {stats['unique_brands']} ä¸ª
- æ¶‰åŠæœç´¢è´¦æˆ·æ•°: {stats['account_count']} ä¸ª

æ–‡ä»¶å·²ä¿å­˜åˆ°: {csv_path}""")
            

            return csv_path
            
        except Exception as e:
            logger.error(f"[DataMergerTool] æ•°æ®æ‹¼æ¥å¤±è´¥: {e}")
            return f"æ•°æ®æ‹¼æ¥å¤±è´¥: {str(e)}"
    
    def _get_search_results(self, keyword: str) -> List[Dict[str, Any]]:
        """ä»xhs_search_resultè¡¨è·å–æŒ‡å®šå…³é”®è¯çš„æœç´¢ç»“æœ"""
        return self.db.get_search_results_by_keyword(keyword)
    
    def _get_note_details(self, note_ids: List[str]) -> List[Dict[str, Any]]:
        """ä»xhs_noteè¡¨è·å–æŒ‡å®šnote_idçš„ç¬”è®°è¯¦æƒ…"""
        return self.db.get_note_details_by_ids(note_ids)
    
    def _merge_multi_account_rankings(self, search_results: List[Dict]) -> Dict[str, Dict]:
        """
        åˆå¹¶å¤šè´¦æˆ·çš„æ’åºç»“æœï¼ˆä½¿ç”¨RRFç®—æ³•ï¼‰
        
        Args:
            search_results: åŸå§‹æœç´¢ç»“æœåˆ—è¡¨
            
        Returns:
            Dict[note_id, {rrf_score, final_rank, account_ranks, account_list}]
        """
        # æŒ‰note_idåˆ†ç»„ï¼Œæ”¶é›†å„è´¦æˆ·çš„æ’å
        note_rankings = {}
        all_accounts = set()
        
        for result in search_results:
            note_id = result.get('note_id')
            account = result.get('search_account')
            rank = result.get('rank')
            
            if not note_id or not account or rank is None:
                continue
                
            all_accounts.add(account)
            
            if note_id not in note_rankings:
                note_rankings[note_id] = {
                    'account_ranks': {},
                    'search_records': []
                }
            
            note_rankings[note_id]['account_ranks'][account] = rank
            note_rankings[note_id]['search_records'].append(result)
        
        all_accounts = sorted(list(all_accounts))
        account_count = len(all_accounts)
        
        logger.info(f"[DataMergerTool] å‘ç° {account_count} ä¸ªæœç´¢è´¦æˆ·: {all_accounts}")
        
        # è®¡ç®—æ¯ä¸ªnote_idçš„RRFåˆ†æ•°
        merged_rankings = {}
        
        for note_id, data in note_rankings.items():
            account_ranks = data['account_ranks']
            
            # æ”¶é›†å„è´¦æˆ·çš„æ’åï¼Œç¼ºå¤±çš„ç”¨Noneè¡¨ç¤º
            ranks = []
            for account in all_accounts:
                ranks.append(account_ranks.get(account))
            
            # è®¡ç®—RRFåˆ†æ•°
            rrf_score = self._calculate_rrf_score(ranks)
            
            merged_rankings[note_id] = {
                'rrf_score': rrf_score,
                'account_ranks': account_ranks,
                'account_list': all_accounts,
                'search_records': data['search_records']
            } 
        
        # RRFç®—æ³•ï¼šåˆ†æ•°è¶Šé«˜è¶Šå¥½ï¼ˆé™åºæ’åˆ—ï¼‰
        sorted_notes = sorted(merged_rankings.items(), 
                            key=lambda x: x[1]['rrf_score'], reverse=True)
        
        # åˆ†é…æœ€ç»ˆæ’å
        for final_rank, (note_id, data) in enumerate(sorted_notes, 1):
            merged_rankings[note_id]['final_rank'] = final_rank
            merged_rankings[note_id]['merged_rank'] = data['rrf_score']  # ä¿æŒå…¼å®¹æ€§
        
        # è®°å½•æ’åºç»“æœ
        logger.info(f"[DataMergerTool] RRFæ’åºç¤ºä¾‹ï¼ˆå‰5åï¼‰:")
        
        for i, (note_id, data) in enumerate(sorted_notes[:5]):
            account_ranks_str = ', '.join([f"{acc}:{data['account_ranks'].get(acc, 'N/A')}" 
                                         for acc in all_accounts])
            
            coverage = len([r for r in [data['account_ranks'].get(acc) for acc in all_accounts] if r is not None])
            
            logger.info(f"  {i+1}. {note_id}: RRFåˆ†æ•°={data['rrf_score']:.4f}, "
                       f"è¦†ç›–è´¦æˆ·={coverage}/{len(all_accounts)}, "
                       f"å„è´¦æˆ·æ’å=[{account_ranks_str}]")
        
        return merged_rankings
    
    def _calculate_rrf_score(self, ranks: List[Optional[int]], k: int = 60) -> float:
        """
        å€’æ•°æ’åèåˆç®—æ³•ï¼ˆReciprocal Rank Fusion - RRFï¼‰
        
        ç®—æ³•åŸç†ï¼š
        RRFæ˜¯ä¸€ç§ç»å…¸çš„å¤šåˆ—è¡¨æ’åèåˆç®—æ³•ï¼Œæœ€åˆç”¨äºä¿¡æ¯æ£€ç´¢é¢†åŸŸã€‚
        å®ƒé€šè¿‡è®¡ç®—å„ä¸ªæ’åçš„å€’æ•°å’Œæ¥èåˆå¤šä¸ªæ’ååˆ—è¡¨ï¼Œè‡ªç„¶åœ°å¹³è¡¡äº†ä¸åŒæ¥æºçš„æƒé‡ã€‚
        
        æ•°å­¦å…¬å¼ï¼š
        RRF_score = Î£(1/(k + rank_i)) for all valid ranks
        
        å…¶ä¸­ï¼š
        - rank_i: å†…å®¹åœ¨ç¬¬iä¸ªè´¦æˆ·ä¸­çš„æ’å
        - k: å¹³æ»‘å¸¸æ•°ï¼Œç”¨äºå‡å°‘æ’åå·®å¼‚çš„å½±å“ï¼Œé€šå¸¸è®¾ä¸º60
        - åªå¯¹æœ‰æ•ˆæ’åï¼ˆéNoneï¼‰è¿›è¡Œæ±‚å’Œ
        
        ç®—æ³•ä¼˜åŠ¿ï¼š
        1. è‡ªç„¶å¹³è¡¡æ€§ï¼šå€’æ•°å‡½æ•°ä½¿å¾—å‰æ’å’Œåæ’çš„å·®è·ç¬¦åˆç›´è§‰
        2. å¤šè´¦æˆ·å‹å¥½ï¼šåœ¨å¤šä¸ªè´¦æˆ·ä¸­å‡ºç°çš„å†…å®¹ä¼šè·å¾—æ›´é«˜çš„æ€»åˆ†
        3. æŠ—å¼‚å¸¸å€¼ï¼šå•ä¸ªè´¦æˆ·çš„æç«¯æ’åä¸ä¼šè¿‡åº¦å½±å“æœ€ç»ˆç»“æœ
        4. æ— å‚æ•°æ•æ„Ÿæ€§ï¼škå€¼çš„é€‰æ‹©å¯¹ç»“æœå½±å“ç›¸å¯¹è¾ƒå°
        
        å­¦æœ¯å‡ºå¤„ï¼š
        - Cormack, G. V., Clarke, C. L., & Buettcher, S. (2009). 
          "Reciprocal rank fusion outperforms condorcet and individual rank learning methods"
          In Proceedings of the 32nd international ACM SIGIR conference
        - å¹¿æ³›åº”ç”¨äºæœç´¢å¼•æ“ç»“æœèåˆã€æ¨èç³»ç»Ÿç­‰é¢†åŸŸ
        
        å®é™…åº”ç”¨åœºæ™¯ï¼š
        é€‚ç”¨äºå°çº¢ä¹¦å¤šè´¦æˆ·æœç´¢ç»“æœåˆå¹¶ï¼Œè§£å†³åŸå§‹ç®—æ³•å¯¹å•è´¦æˆ·é«˜æ’åè¿‡äºå‹å¥½çš„é—®é¢˜ã€‚
        
        Args:
            ranks: å„è´¦æˆ·çš„æ’ååˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºæœªå‡ºç°
            k: RRFå¸¸æ•°ï¼Œé€šå¸¸è®¾ä¸º60ï¼Œç”¨äºå¹³æ»‘æ’åå·®å¼‚
            
        Returns:
            RRFåˆ†æ•°ï¼ˆè¶Šé«˜è¶Šå¥½ï¼Œéœ€è¦æŒ‰é™åºæ’åˆ—ï¼‰
            
        Example:
            ranks = [1, None, None, None]  # å•è´¦æˆ·æ’å1
            rrf_score = 1/(60+1) = 0.0164
            
            ranks = [5, 7, 9, None]  # å¤šè´¦æˆ·æ’å5,7,9  
            rrf_score = 1/(60+5) + 1/(60+7) + 1/(60+9) = 0.0448
            
            ç»“æœï¼šå¤šè´¦æˆ·ç¨³å®šæ’åè·å¾—æ›´é«˜åˆ†æ•°
        """
        valid_ranks = [r for r in ranks if r is not None]
        
        if not valid_ranks:
            # å®Œå…¨æ²¡æœ‰å‡ºç°çš„å†…å®¹ï¼Œè¿”å›æœ€ä½åˆ†æ•°
            return 0.0
        
        # è®¡ç®—RRFåˆ†æ•°ï¼šå¯¹æ‰€æœ‰æœ‰æ•ˆæ’åè®¡ç®—å€’æ•°å’Œ
        # å…¬å¼ï¼šRRF_score = Î£(1/(k + rank_i))
        rrf_score = sum(1.0 / (k + rank) for rank in valid_ranks)
        
        # RRFåˆ†æ•°è¶Šé«˜æ’åè¶Šå¥½
        return rrf_score
    
    def _merge_data_with_rankings(self, merged_rankings: Dict, note_details: List[Dict], keyword: str) -> List[Dict]:
        """
        ä½¿ç”¨RRFæ’åºç»“æœæ‹¼æ¥æ•°æ®
        
        Args:
            merged_rankings: RRFæ’åºç»“æœ
            note_details: ç¬”è®°è¯¦æƒ…åˆ—è¡¨
            keyword: å…³é”®è¯
            
        Returns:
            æ‹¼æ¥åçš„æ•°æ®åˆ—è¡¨
        """
        # å°†ç¬”è®°è¯¦æƒ…è½¬æ¢ä¸ºå­—å…¸ï¼Œä»¥note_idä¸ºkey
        note_dict = {note['note_id']: note for note in note_details}
        
        merged_data = []
        
        # æŒ‰æœ€ç»ˆæ’åæ’åºå¤„ç†
        sorted_rankings = sorted(merged_rankings.items(), key=lambda x: x[1]['final_rank'])
        
        for note_id, ranking_data in sorted_rankings:
            note_detail = note_dict.get(note_id, {})
            
            # å¯¹å“ç‰Œç›¸å…³å­—æ®µè¿›è¡Œæ ‡å‡†åŒ–å¤„ç†
            normalized_brand_data = self._normalize_brand_fields(note_detail)
            
            # è·å–ç¬¬ä¸€ä¸ªæœç´¢è®°å½•ä½œä¸ºä»£è¡¨ï¼ˆç”¨äºè·å–åŸºæœ¬æœç´¢ä¿¡æ¯ï¼‰
            representative_search = ranking_data['search_records'][0] if ranking_data['search_records'] else {}
            
            # æ„å»ºè´¦æˆ·æ’åä¿¡æ¯å­—ç¬¦ä¸²
            account_ranks_info = []
            for account in ranking_data['account_list']:
                rank = ranking_data['account_ranks'].get(account)
                account_ranks_info.append(f"{account}:{rank if rank else 'N/A'}")
            
            # åˆå¹¶è®°å½•
            merged_record = {
                # æœç´¢ç»“æœå­—æ®µï¼ˆä½¿ç”¨ä»£è¡¨æ€§è®°å½•ï¼‰
                'search_id': representative_search.get('id'),
                'keyword': keyword,
                'rank': ranking_data['final_rank'],  # æœ€ç»ˆæ’å
                'rrf_score': round(ranking_data['rrf_score'], 4),  # RRFåˆ†æ•°
                'account_ranks': '; '.join(account_ranks_info),  # å„è´¦æˆ·æ’åè¯¦æƒ…
                'note_id': note_id,
                
                # ç¬”è®°è¯¦æƒ…å­—æ®µ
                'title': note_detail.get('title'),
                'type': note_detail.get('type'),
                'desc': note_detail.get('desc'),
                'note_url': note_detail.get('note_url'),
                'video_url': note_detail.get('video_url'),
                'image_list': note_detail.get('image_list'),
                'tag_list': note_detail.get('tag_list'),
                'author_id': note_detail.get('author_id'),
                'nickname': note_detail.get('nickname'),
                'last_update_time': note_detail.get('last_update_time'),
                'liked_count': note_detail.get('liked_count'),
                'collected_count': note_detail.get('collected_count'),
                'comment_count': note_detail.get('comment_count'),
                'share_count': note_detail.get('share_count'),
                
                # ä½¿ç”¨æ ‡å‡†åŒ–åçš„å“ç‰Œæ•°æ®
                'brand_list': normalized_brand_data['brand_list'],
                'spu_list': note_detail.get('spu_list'),
                'emotion_dict': normalized_brand_data['emotion_dict'],
                'evaluation_dict': normalized_brand_data['evaluation_dict'],
                
                # æ•°æ®çŠ¶æ€æ ‡è¯†
                'has_note_detail': bool(note_detail),
                'has_brand_info': self._has_valid_brand_info(normalized_brand_data['brand_list']),
                'data_crawler_time': representative_search.get('created_at'),
        
            }
            
            merged_data.append(merged_record)
        
        return merged_data
    
    def _normalize_brand_fields(self, note_detail: Dict) -> Dict:
        """æ ‡å‡†åŒ–å“ç‰Œç›¸å…³å­—æ®µ"""
        result = {
            'brand_list': note_detail.get('brand_list'),
            'emotion_dict': note_detail.get('emotion_dict'),
            'evaluation_dict': note_detail.get('evaluation_dict')
        }
        
        if not note_detail:
            return result
        
        try:
            # 1. æ ‡å‡†åŒ–å“ç‰Œåˆ—è¡¨
            brand_list = self._parse_json_field(note_detail.get('brand_list'))
            if isinstance(brand_list, list) and brand_list:
                normalized_brands = []
                brand_mapping = {}  # åŸå§‹å“ç‰Œå -> æ ‡å‡†åŒ–å“ç‰Œåçš„æ˜ å°„
                
                for original_brand in brand_list:
                    if original_brand and isinstance(original_brand, str):
                        normalized_brand = self.brand_normalizer.normalize_brand_name(original_brand.strip())
                        if normalized_brand:
                            normalized_brands.append(normalized_brand)
                            brand_mapping[original_brand] = normalized_brand
                
                # å»é‡ä½†ä¿æŒé¡ºåº
                seen = set()
                unique_normalized_brands = []
                for brand in normalized_brands:
                    if brand not in seen:
                        unique_normalized_brands.append(brand)
                        seen.add(brand)
                
                result['brand_list'] = unique_normalized_brands
                
                # 2. æ›´æ–°æƒ…æ„Ÿå­—å…¸ä¸­çš„å“ç‰Œå
                emotion_dict = self._parse_json_field(note_detail.get('emotion_dict'))
                if isinstance(emotion_dict, dict) and emotion_dict:
                    normalized_emotion_dict = {}
                    for original_brand, emotion_info in emotion_dict.items():
                        # æŸ¥æ‰¾å¯¹åº”çš„æ ‡å‡†åŒ–å“ç‰Œå
                        normalized_brand = brand_mapping.get(original_brand, 
                                                           self.brand_normalizer.normalize_brand_name(original_brand))
                        if normalized_brand:
                            normalized_emotion_dict[normalized_brand] = emotion_info
                    result['emotion_dict'] = normalized_emotion_dict
                
                # 3. æ›´æ–°è¯„ä»·å­—å…¸ä¸­çš„å“ç‰Œå
                evaluation_dict = self._parse_json_field(note_detail.get('evaluation_dict'))
                if isinstance(evaluation_dict, dict) and evaluation_dict:
                    normalized_evaluation_dict = {}
                    for original_brand, evaluation_info in evaluation_dict.items():
                        # æŸ¥æ‰¾å¯¹åº”çš„æ ‡å‡†åŒ–å“ç‰Œå
                        normalized_brand = brand_mapping.get(original_brand,
                                                           self.brand_normalizer.normalize_brand_name(original_brand))
                        if normalized_brand:
                            normalized_evaluation_dict[normalized_brand] = evaluation_info
                    result['evaluation_dict'] = normalized_evaluation_dict
                
                # è®°å½•æ ‡å‡†åŒ–ç»“æœ
                if brand_mapping:
                    original_brands = list(brand_mapping.keys())
                    normalized_brands = list(brand_mapping.values())
                    logger.debug(f"[DataMergerTool] å“ç‰Œæ ‡å‡†åŒ–: {original_brands} -> {normalized_brands}")
            
        except Exception as e:
            logger.warning(f"[DataMergerTool] å“ç‰Œæ ‡å‡†åŒ–å¤±è´¥: {e}")
            # å¦‚æœæ ‡å‡†åŒ–å¤±è´¥ï¼Œè¿”å›åŸå§‹æ•°æ®
            pass
        
        return result
    
    def _parse_json_field(self, field_value: Any) -> Any:
        """å®‰å…¨è§£æJSONå­—æ®µ"""
        if pd.isna(field_value):
            return []
        
        try:
            if isinstance(field_value, str):
                return self.file_manager.parse_json_string(field_value)
            return field_value
        except (ValueError, TypeError):
            return []
    
    def _has_valid_brand_info(self, brand_list) -> bool:
        """åˆ¤æ–­æ˜¯å¦æœ‰æœ‰æ•ˆçš„å“ç‰Œä¿¡æ¯"""
        if not brand_list:  # None, '', False, 0 ç­‰falsyå€¼
            return False
        
        # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•è§£æä¸ºJSON
        if isinstance(brand_list, str):
            try:
                parsed_list = json.loads(brand_list)
                return isinstance(parsed_list, list) and len(parsed_list) > 0
            except (json.JSONDecodeError, TypeError):
                return False
        
        # å¦‚æœå·²ç»æ˜¯åˆ—è¡¨ï¼Œæ£€æŸ¥æ˜¯å¦éç©º
        if isinstance(brand_list, list):
            return len(brand_list) > 0
        
        # å…¶ä»–æƒ…å†µè¿”å›False
        return False
    
    def _save_to_csv(self, data: List[Dict], keyword: str, output_dir_inner: str, output_dir_outer: str) -> str:
        """ä¿å­˜æ•°æ®åˆ°CSVæ–‡ä»¶ï¼ˆåªä¿å­˜å‰100åï¼‰"""
        
        # ç”Ÿæˆæ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d")
        output_dir_inner = self.file_manager.build_path(output_dir_inner, keyword)
        output_dir_outer = self.file_manager.build_path(output_dir_outer, keyword)
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        self.file_manager.ensure_directory(output_dir_inner)
        self.file_manager.ensure_directory(output_dir_outer)
        
        # æŒ‰rankæ’åºå¹¶åªå–å‰100å
        sorted_data = sorted(data, key=lambda x: x.get('rank', float('inf')))
        top_100_data = sorted_data[:100]
        
        logger.info(f"[DataMergerTool] åŸå§‹æ•°æ®{len(data)}æ¡ï¼Œç­›é€‰å‰100ååä¸º{len(top_100_data)}æ¡")
        
        filename = f"merged_data_{timestamp}.csv"
        filepath = self.file_manager.build_path(output_dir_inner, filename)
        
        # è½¬æ¢ä¸ºDataFrameå¹¶ä¿å­˜
        df = pd.DataFrame(top_100_data)
        
        # å¤„ç†JSONå­—æ®µï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²
        json_columns = ['image_list', 'tag_list', 'brand_list', 'spu_list', 'emotion_dict', 'evaluation_dict']
        for col in json_columns:
            if col in df.columns:
                df[col] = df[col].apply(lambda x: self.file_manager.to_json_string(x, ensure_ascii=False) if x else '')
        
        # ç”Ÿæˆå¯¹å¤–è¾“å‡ºçš„ä¸­æ–‡CSVæ–‡ä»¶
        outer_filename = f"basic_data_{timestamp}.csv"
        outer_filepath = self.file_manager.build_path(output_dir_outer, outer_filename)
        
        # ç­›é€‰column_mappingä¸­å­˜åœ¨çš„åˆ—å¹¶é‡å‘½åä¸ºä¸­æ–‡
        available_columns = [col for col in self.column_mapping.keys() if col in df.columns]
        df_chinese = df[available_columns].copy()
        df_chinese.rename(columns=self.column_mapping, inplace=True)

        # ä¿å­˜CSV
        self.file_manager.save_csv(df, filepath)
        self.file_manager.save_csv(df_chinese, outer_filepath)
        
        logger.info(f"[DataMergerTool] å†…éƒ¨æ•°æ®å·²ä¿å­˜åˆ°: {filepath}")
        logger.info(f"[DataMergerTool] å¯¹å¤–æ•°æ®å·²ä¿å­˜åˆ°: {outer_filepath}")
        return filepath
    
    def _generate_statistics(self, merged_data: List[Dict], keyword: str) -> Dict[str, Any]:
        """ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯"""
        total_count = len(merged_data)
        matched_count = sum(1 for record in merged_data if record['has_note_detail'])
        unmatched_count = total_count - matched_count
        with_brand_count = sum(1 for record in merged_data if record['has_brand_info'])
        
        # è®¡ç®—å”¯ä¸€å“ç‰Œæ•°
        all_brands = set()
        for record in merged_data:
            if record.get('brand_list'):
                try:
                    brands = record['brand_list'] if isinstance(record['brand_list'], list) else json.loads(record['brand_list'] or '[]')
                    all_brands.update(brands)
                except:
                    pass
        
        # è®¡ç®—å¹³å‡RRFåˆ†æ•°
        rrf_scores = [record['rrf_score'] for record in merged_data if record.get('rrf_score') is not None]
        avg_rrf_score = sum(rrf_scores) / len(rrf_scores) if rrf_scores else 0
        
        # è®¡ç®—è´¦æˆ·è¦†ç›–ç»Ÿè®¡
        account_coverage = []
        for record in merged_data:
            account_ranks = record.get('account_ranks', '')
            if account_ranks:
                # ç»Ÿè®¡æœ‰æ’åçš„è´¦æˆ·æ•°ï¼ˆä¸æ˜¯N/Açš„ï¼‰
                ranks_list = account_ranks.split('; ')
                coverage = sum(1 for rank_info in ranks_list if not rank_info.endswith(':N/A'))
                account_coverage.append(coverage)
        
        avg_account_coverage = sum(account_coverage) / len(account_coverage) if account_coverage else 0
        max_account_count = max(account_coverage) if account_coverage else 0
        
        return {
            'matched_count': matched_count,
            'unmatched_count': unmatched_count,
            'with_brand_count': with_brand_count,
            'unique_brands': len(all_brands),
            'avg_rrf_score': avg_rrf_score,
            'account_count': max_account_count,  # æœ€å¤§è´¦æˆ·è¦†ç›–æ•°
            'avg_account_per_note': avg_account_coverage  # å¹³å‡æ¯ä¸ªç¬”è®°çš„è´¦æˆ·è¦†ç›–æ•°
        } 