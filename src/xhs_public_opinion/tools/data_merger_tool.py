import os
import json
import pandas as pd
from typing import List, Dict, Any, Optional
from crewai.tools import BaseTool
from supabase import create_client, Client
import logging
from datetime import datetime
from dotenv import load_dotenv
from .brand_normalizer import get_brand_normalizer
from .brand_normalizer import BrandNormalizer

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DataMergerTool(BaseTool):
    """æ•°æ®æ‹¼æ¥å·¥å…· - å°†æœç´¢ç»“æœè¡¨å’Œç¬”è®°è¯¦æƒ…è¡¨è¿æ¥ç”Ÿæˆå®½è¡¨"""
    name: str = "data_merger"
    description: str = "æ ¹æ®æŒ‡å®šå…³é”®è¯ï¼Œå°†xhs_search_resultå’Œxhs_noteè¡¨æ ¹æ®note_idè¿æ¥ï¼Œç”ŸæˆCSVå®½è¡¨æ–‡ä»¶"
    
    # å£°æ˜Pydanticå­—æ®µ
    url: Optional[str] = None
    key: Optional[str] = None
    client: Optional[Client] = None
    brand_normalizer: Optional[BrandNormalizer] = None
    column_mapping: Dict[str, str] = {}

    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.url = os.getenv("SEO_SUPABASE_URL")
        self.key = os.getenv("SEO_SUPABASE_ANON_KEY")
        
        if not self.url or not self.key:
            raise ValueError("è¯·ç¡®ä¿è®¾ç½®äº† SEO_SUPABASE_URL å’Œ SEO_SUPABASE_ANON_KEY ç¯å¢ƒå˜é‡")
        
        self.client = create_client(self.url, self.key)
        
        # åˆå§‹åŒ–å“ç‰Œæ ‡å‡†åŒ–å™¨
        self.brand_normalizer = get_brand_normalizer()

        self.column_mapping = {
        "keyword": "æœç´¢å…³é”®è¯",
        "rank": "æœç´¢æ’å",
        "note_id": "ç¬”è®°ID",
        "type": "ç¬”è®°ç±»å‹",
        "title": "ç¬”è®°æ ‡é¢˜",
        "desc": "ç¬”è®°æè¿°",
        "note_url": "ç¬”è®°URL",
        "author_id": "ä½œè€…ID",
        "nickname": "ä½œè€…æ˜µç§°",
        "last_update_time": "ç¬”è®°æœ€åæ›´æ–°æ—¶é—´",
        "liked_count": "ç‚¹èµæ•°",
        "collected_count": "æ”¶è—æ•°",
        "comment_count": "è¯„è®ºæ•°",
        "share_count": "åˆ†äº«æ•°",
        "brand_list": "å“ç‰Œåˆ—è¡¨",
        "spu_list": "SPUåˆ—è¡¨",
        "emotion_dict": "æƒ…æ„Ÿå€¾å‘",
        "evaluation_dict": "é«˜é¢‘è¯",
        "data_crawler_time": "æ•°æ®é‡‡é›†æ—¶é—´",
    }
        
    
    def _run(self, keyword: str, output_dir_inner: str = "data/export",  output_dir_outer: str = "outputs") -> str:
        """
        æ‰§è¡Œæ•°æ®æ‹¼æ¥ä»»åŠ¡
        
        Args:
            keyword: è¦æŸ¥è¯¢çš„å…³é”®è¯
            output_dir: è¾“å‡ºç›®å½•ï¼Œé»˜è®¤ä¸ºdata/export
            
        Returns:
            è¿”å›æ“ä½œç»“æœä¿¡æ¯
        """
        try:
            logger.info(f"[DataMergerTool] å¼€å§‹ä¸ºå…³é”®è¯ '{keyword}' è¿›è¡Œæ•°æ®æ‹¼æ¥")
            
            # 1. ä»xhs_search_resultè¡¨è·å–æŒ‡å®šå…³é”®è¯çš„æœç´¢ç»“æœ
            search_results = self._get_search_results(keyword)
            if not search_results:
                return f"å…³é”®è¯ '{keyword}' æ²¡æœ‰æ‰¾åˆ°æœç´¢ç»“æœæ•°æ®"
            
            logger.info(f"[DataMergerTool] è·å–åˆ° {len(search_results)} æ¡æœç´¢ç»“æœ")
            
            # 2. åˆå¹¶å¤šè´¦æˆ·æ’åºç»“æœ
            merged_rankings = self._merge_multi_account_rankings(search_results)
            logger.info(f"[DataMergerTool] åˆå¹¶åå¾—åˆ° {len(merged_rankings)} ä¸ªå”¯ä¸€note_idçš„æ’åº")
            
            # 3. æå–æ‰€æœ‰note_id
            note_ids = list(merged_rankings.keys())
            
            logger.info(f"[DataMergerTool] æå–åˆ° {len(note_ids)} ä¸ªå”¯ä¸€çš„note_id")
            
            # 4. ä»xhs_noteè¡¨è·å–å¯¹åº”çš„ç¬”è®°è¯¦æƒ…
            note_details = self._get_note_details(note_ids)
            
            logger.info(f"[DataMergerTool] è·å–åˆ° {len(note_details)} æ¡ç¬”è®°è¯¦æƒ…")
            
            # 5. æ•°æ®æ‹¼æ¥ï¼ˆä½¿ç”¨åˆå¹¶åçš„æ’åºç»“æœï¼‰
            merged_data = self._merge_data_with_rankings(merged_rankings, note_details, keyword)
            
            # 6. ç”ŸæˆCSVæ–‡ä»¶ï¼ˆåªä¿å­˜å‰100åï¼‰
            csv_path = self._save_to_csv(merged_data, keyword, output_dir_inner, output_dir_outer)
            
            # 7. ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Šï¼ˆåŸºäºå‰100åæ•°æ®ï¼‰
            # æŒ‰rankæ’åºå¹¶åªå–å‰100åç”¨äºç»Ÿè®¡
            sorted_data = sorted(merged_data, key=lambda x: x.get('rank', float('inf')))
            top_100_data = sorted_data[:100]
            stats = self._generate_statistics(top_100_data, keyword)
            logger.info(f"""âœ… æ•°æ®æ‹¼æ¥å®Œæˆï¼

ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:
- å…³é”®è¯: {keyword}
- åŸå§‹æœç´¢ç»“æœè®°å½•: {len(search_results)} æ¡
- åˆå¹¶åå”¯ä¸€ç¬”è®°æ•°: {len(note_ids)} ä¸ª
- ç­›é€‰å‰100åè®°å½•: {len(top_100_data)} æ¡
- æˆåŠŸåŒ¹é…: {stats['matched_count']} æ¡
- æœªåŒ¹é…: {stats['unmatched_count']} æ¡
- è¾“å‡ºæ–‡ä»¶: {csv_path}

ğŸ“ˆ æ•°æ®æ¦‚è§ˆï¼ˆå‰100åï¼‰:
- æ€»è®°å½•æ•°: {len(top_100_data)}
- åŒ…å«å“ç‰Œä¿¡æ¯çš„è®°å½•: {stats['with_brand_count']} æ¡
- æ¶‰åŠå“ç‰Œæ•°: {stats['unique_brands']} ä¸ª
- æ¶‰åŠæœç´¢è´¦æˆ·æ•°: {stats['account_count']} ä¸ª

æ–‡ä»¶å·²ä¿å­˜åˆ°: {csv_path}""")
            

            return csv_path
            
        except Exception as e:
            logger.error(f"[DataMergerTool] æ•°æ®æ‹¼æ¥å¤±è´¥: {e}")
            return f"æ•°æ®æ‹¼æ¥å¤±è´¥: {str(e)}"
    
    def _get_search_results(self, keyword: str) -> List[Dict[str, Any]]:
        """ä»xhs_search_resultè¡¨è·å–æŒ‡å®šå…³é”®è¯çš„æœç´¢ç»“æœ"""
        try:
            response = (
                self.client.table("xhs_search_result")
                .select("*")
                .eq("keyword", keyword)
                .order("rank", desc=False)  # æŒ‰æ’åå‡åºæ’åˆ—
                .execute()
            )
            return response.data
        except Exception as e:
            logger.error(f"è·å–æœç´¢ç»“æœå¤±è´¥: {e}")
            return []
    
    def _get_note_details(self, note_ids: List[str]) -> List[Dict[str, Any]]:
        """ä»xhs_noteè¡¨è·å–æŒ‡å®šnote_idçš„ç¬”è®°è¯¦æƒ…"""
        if not note_ids:
            return []
        
        try:
            # Supabaseçš„inæ“ä½œ
            response = (
                self.client.table("xhs_note")
                .select("*")
                .in_("note_id", note_ids)
                .execute()
            )
            return response.data
        except Exception as e:
            logger.error(f"è·å–ç¬”è®°è¯¦æƒ…å¤±è´¥: {e}")
            return []
    
    def _merge_multi_account_rankings(self, search_results: List[Dict]) -> Dict[str, Dict]:
        """
        åˆå¹¶å¤šè´¦æˆ·çš„æ’åºç»“æœ
        
        Args:
            search_results: åŸå§‹æœç´¢ç»“æœåˆ—è¡¨
            
        Returns:
            Dict[note_id, {merged_rank, final_rank, account_ranks, account_list}]
        """
        # æŒ‰note_idåˆ†ç»„ï¼Œæ”¶é›†å„è´¦æˆ·çš„æ’å
        note_rankings = {}
        all_accounts = set()
        
        for result in search_results:
            note_id = result.get('note_id')
            account = result.get('search_account')
            rank = result.get('rank')
            
            if not note_id or not account or rank is None:
                continue
                
            all_accounts.add(account)
            
            if note_id not in note_rankings:
                note_rankings[note_id] = {
                    'account_ranks': {},
                    'search_records': []
                }
            
            note_rankings[note_id]['account_ranks'][account] = rank
            note_rankings[note_id]['search_records'].append(result)
        
        all_accounts = sorted(list(all_accounts))
        account_count = len(all_accounts)
        max_rank = 110  # æœç´¢ç»“æœæœ€å¤§æ’å
        
        logger.info(f"[DataMergerTool] å‘ç° {account_count} ä¸ªæœç´¢è´¦æˆ·: {all_accounts}")
        
        # è®¡ç®—æ¯ä¸ªnote_idçš„åˆå¹¶æ’å
        merged_rankings = {}
        
        for note_id, data in note_rankings.items():
            account_ranks = data['account_ranks']
            
            # æ”¶é›†å„è´¦æˆ·çš„æ’åï¼Œç¼ºå¤±çš„ç”¨Noneè¡¨ç¤º
            ranks = []
            for account in all_accounts:
                ranks.append(account_ranks.get(account))
            # è®¡ç®—åˆå¹¶æ’å
            merged_rank = self._calculate_merged_rank(ranks, max_rank)
            
            merged_rankings[note_id] = {
                'merged_rank': merged_rank,
                'account_ranks': account_ranks,
                'account_list': all_accounts,
                'search_records': data['search_records']
            } 
        
        # æŒ‰åˆå¹¶æ’åæ’åºï¼Œåˆ†é…æœ€ç»ˆæ’å
        sorted_notes = sorted(merged_rankings.items(), key=lambda x: x[1]['merged_rank'])
        
        for final_rank, (note_id, data) in enumerate(sorted_notes, 1):
            merged_rankings[note_id]['final_rank'] = final_rank
        
        # è®°å½•æ’åºç»“æœ
        logger.info(f"[DataMergerTool] æ’åºç¤ºä¾‹ï¼ˆå‰5åï¼‰:")
        for i, (note_id, data) in enumerate(sorted_notes[:5]):
            account_ranks_str = ', '.join([f"{acc}:{data['account_ranks'].get(acc, 'N/A')}" 
                                         for acc in all_accounts])
            logger.info(f"  {i+1}. {note_id}: åˆå¹¶æ’å={data['merged_rank']:.2f}, å„è´¦æˆ·æ’å=[{account_ranks_str}]")
        
        return merged_rankings
    
    def _calculate_merged_rank(self, ranks: List[Optional[int]], max_rank: int = 110) -> float:
        """
        è®¡ç®—åˆå¹¶æ’åï¼ˆåŠ æƒå¹³å‡ + æƒ©ç½šå› å­ç®—æ³•ï¼‰
        
        Args:
            ranks: å„è´¦æˆ·çš„æ’ååˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºæœªå‡ºç°
            max_rank: æœ€å¤§æ’åå€¼
            
        Returns:
            åˆå¹¶åçš„æ’ååˆ†æ•°
        """
        valid_ranks = [r for r in ranks if r is not None]
        missing_count = len(ranks) - len(valid_ranks)
        
        if not valid_ranks:
            # å®Œå…¨æ²¡æœ‰å‡ºç°çš„å†…å®¹ï¼Œç»™äºˆæœ€å·®æ’å
            return max_rank * 2
        
        # æ–¹æ³•ï¼šåŠ æƒå¹³å‡ + æƒ©ç½šå› å­
        avg_rank = sum(valid_ranks) / len(valid_ranks)
        
        # å¯¹ç¼ºå¤±æ•°æ®æ·»åŠ æƒ©ç½šï¼šæ¯ç¼ºå¤±ä¸€ä¸ªè´¦æˆ·ï¼Œæ’ååç§»15%
        penalty_rate = 0.15
        penalty = missing_count * penalty_rate * avg_rank
        
        merged_rank = avg_rank + penalty
        
        return merged_rank
    
    def _merge_data_with_rankings(self, merged_rankings: Dict, note_details: List[Dict], keyword: str) -> List[Dict]:
        """
        ä½¿ç”¨åˆå¹¶æ’åºç»“æœæ‹¼æ¥æ•°æ®
        
        Args:
            merged_rankings: åˆå¹¶æ’åºç»“æœ
            note_details: ç¬”è®°è¯¦æƒ…åˆ—è¡¨
            keyword: å…³é”®è¯
            
        Returns:
            æ‹¼æ¥åçš„æ•°æ®åˆ—è¡¨
        """
        # å°†ç¬”è®°è¯¦æƒ…è½¬æ¢ä¸ºå­—å…¸ï¼Œä»¥note_idä¸ºkey
        note_dict = {note['note_id']: note for note in note_details}
        
        merged_data = []
        
        # æŒ‰æœ€ç»ˆæ’åæ’åºå¤„ç†
        sorted_rankings = sorted(merged_rankings.items(), key=lambda x: x[1]['final_rank'])
        
        for note_id, ranking_data in sorted_rankings:
            note_detail = note_dict.get(note_id, {})
            
            # å¯¹å“ç‰Œç›¸å…³å­—æ®µè¿›è¡Œæ ‡å‡†åŒ–å¤„ç†
            normalized_brand_data = self._normalize_brand_fields(note_detail)
            
            # è·å–ç¬¬ä¸€ä¸ªæœç´¢è®°å½•ä½œä¸ºä»£è¡¨ï¼ˆç”¨äºè·å–åŸºæœ¬æœç´¢ä¿¡æ¯ï¼‰
            representative_search = ranking_data['search_records'][0] if ranking_data['search_records'] else {}
            
            # æ„å»ºè´¦æˆ·æ’åä¿¡æ¯å­—ç¬¦ä¸²
            account_ranks_info = []
            for account in ranking_data['account_list']:
                rank = ranking_data['account_ranks'].get(account)
                account_ranks_info.append(f"{account}:{rank if rank else 'N/A'}")
            
            # åˆå¹¶è®°å½•
            merged_record = {
                # æœç´¢ç»“æœå­—æ®µï¼ˆä½¿ç”¨ä»£è¡¨æ€§è®°å½•ï¼‰
                'search_id': representative_search.get('id'),
                'keyword': keyword,
                #'search_account': 'MERGED',  # æ ‡è¯†ä¸ºåˆå¹¶ç»“æœ
                #'account_count': len(ranking_data['account_ranks']),  # å‡ºç°åœ¨å¤šå°‘ä¸ªè´¦æˆ·ä¸­
                'rank': ranking_data['final_rank'],  # æœ€ç»ˆæ’å
                #'merged_rank': round(ranking_data['merged_rank'], 2),  # åˆå¹¶æ’ååˆ†æ•°
                #'account_ranks': '; '.join(account_ranks_info),  # å„è´¦æˆ·æ’åè¯¦æƒ…
                'note_id': note_id,
                
                # ç¬”è®°è¯¦æƒ…å­—æ®µ
                'title': note_detail.get('title'),
                'type': note_detail.get('type'),
                'desc': note_detail.get('desc'),
                'note_url': note_detail.get('note_url'),
                'video_url': note_detail.get('video_url'),
                'image_list': note_detail.get('image_list'),
                'tag_list': note_detail.get('tag_list'),
                'author_id': note_detail.get('author_id'),
                'nickname': note_detail.get('nickname'),
                'last_update_time': note_detail.get('last_update_time'),
                'liked_count': note_detail.get('liked_count'),
                'collected_count': note_detail.get('collected_count'),
                'comment_count': note_detail.get('comment_count'),
                'share_count': note_detail.get('share_count'),
                
                # ä½¿ç”¨æ ‡å‡†åŒ–åçš„å“ç‰Œæ•°æ®
                'brand_list': normalized_brand_data['brand_list'],
                'spu_list': note_detail.get('spu_list'),
                'emotion_dict': normalized_brand_data['emotion_dict'],
                'evaluation_dict': normalized_brand_data['evaluation_dict'],
                
                # æ•°æ®çŠ¶æ€æ ‡è¯†
                'has_note_detail': bool(note_detail),
                'has_brand_info': self._has_valid_brand_info(normalized_brand_data['brand_list']),
                'data_crawler_time': representative_search.get('created_at'),
        
            }
            
            merged_data.append(merged_record)
        
        return merged_data
    
    def _normalize_brand_fields(self, note_detail: Dict) -> Dict:
        """æ ‡å‡†åŒ–å“ç‰Œç›¸å…³å­—æ®µ"""
        result = {
            'brand_list': note_detail.get('brand_list'),
            'emotion_dict': note_detail.get('emotion_dict'),
            'evaluation_dict': note_detail.get('evaluation_dict')
        }
        
        if not note_detail:
            return result
        
        try:
            # 1. æ ‡å‡†åŒ–å“ç‰Œåˆ—è¡¨
            brand_list = self._parse_json_field(note_detail.get('brand_list'))
            if isinstance(brand_list, list) and brand_list:
                normalized_brands = []
                brand_mapping = {}  # åŸå§‹å“ç‰Œå -> æ ‡å‡†åŒ–å“ç‰Œåçš„æ˜ å°„
                
                for original_brand in brand_list:
                    if original_brand and isinstance(original_brand, str):
                        normalized_brand = self.brand_normalizer.normalize_brand_name(original_brand.strip())
                        if normalized_brand:
                            normalized_brands.append(normalized_brand)
                            brand_mapping[original_brand] = normalized_brand
                
                # å»é‡ä½†ä¿æŒé¡ºåº
                seen = set()
                unique_normalized_brands = []
                for brand in normalized_brands:
                    if brand not in seen:
                        unique_normalized_brands.append(brand)
                        seen.add(brand)
                
                result['brand_list'] = unique_normalized_brands
                
                # 2. æ›´æ–°æƒ…æ„Ÿå­—å…¸ä¸­çš„å“ç‰Œå
                emotion_dict = self._parse_json_field(note_detail.get('emotion_dict'))
                if isinstance(emotion_dict, dict) and emotion_dict:
                    normalized_emotion_dict = {}
                    for original_brand, emotion_info in emotion_dict.items():
                        # æŸ¥æ‰¾å¯¹åº”çš„æ ‡å‡†åŒ–å“ç‰Œå
                        normalized_brand = brand_mapping.get(original_brand, 
                                                           self.brand_normalizer.normalize_brand_name(original_brand))
                        if normalized_brand:
                            normalized_emotion_dict[normalized_brand] = emotion_info
                    result['emotion_dict'] = normalized_emotion_dict
                
                # 3. æ›´æ–°è¯„ä»·å­—å…¸ä¸­çš„å“ç‰Œå
                evaluation_dict = self._parse_json_field(note_detail.get('evaluation_dict'))
                if isinstance(evaluation_dict, dict) and evaluation_dict:
                    normalized_evaluation_dict = {}
                    for original_brand, evaluation_info in evaluation_dict.items():
                        # æŸ¥æ‰¾å¯¹åº”çš„æ ‡å‡†åŒ–å“ç‰Œå
                        normalized_brand = brand_mapping.get(original_brand,
                                                           self.brand_normalizer.normalize_brand_name(original_brand))
                        if normalized_brand:
                            normalized_evaluation_dict[normalized_brand] = evaluation_info
                    result['evaluation_dict'] = normalized_evaluation_dict
                
                # è®°å½•æ ‡å‡†åŒ–ç»“æœ
                if brand_mapping:
                    original_brands = list(brand_mapping.keys())
                    normalized_brands = list(brand_mapping.values())
                    logger.debug(f"[DataMergerTool] å“ç‰Œæ ‡å‡†åŒ–: {original_brands} -> {normalized_brands}")
            
        except Exception as e:
            logger.warning(f"[DataMergerTool] å“ç‰Œæ ‡å‡†åŒ–å¤±è´¥: {e}")
            # å¦‚æœæ ‡å‡†åŒ–å¤±è´¥ï¼Œè¿”å›åŸå§‹æ•°æ®
            pass
        
        return result
    
    def _parse_json_field(self, field_value: Any) -> Any:
        """å®‰å…¨è§£æJSONå­—æ®µ"""
        if isinstance(field_value, str):
            try:
                return json.loads(field_value)
            except (json.JSONDecodeError, TypeError):
                return field_value
        return field_value
    
    def _has_valid_brand_info(self, brand_list) -> bool:
        """åˆ¤æ–­æ˜¯å¦æœ‰æœ‰æ•ˆçš„å“ç‰Œä¿¡æ¯"""
        if not brand_list:  # None, '', False, 0 ç­‰falsyå€¼
            return False
        
        # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•è§£æä¸ºJSON
        if isinstance(brand_list, str):
            try:
                parsed_list = json.loads(brand_list)
                return isinstance(parsed_list, list) and len(parsed_list) > 0
            except (json.JSONDecodeError, TypeError):
                return False
        
        # å¦‚æœå·²ç»æ˜¯åˆ—è¡¨ï¼Œæ£€æŸ¥æ˜¯å¦éç©º
        if isinstance(brand_list, list):
            return len(brand_list) > 0
        
        # å…¶ä»–æƒ…å†µè¿”å›False
        return False
    
    def _save_to_csv(self, data: List[Dict], keyword: str, output_dir_inner: str, output_dir_outer: str) -> str:
        """ä¿å­˜æ•°æ®åˆ°CSVæ–‡ä»¶ï¼ˆåªä¿å­˜å‰100åï¼‰"""
        
        # ç”Ÿæˆæ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d")
        output_dir_inner = output_dir_inner + "/" + keyword 
        output_dir_outer = output_dir_outer + "/" + keyword 
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(output_dir_inner, exist_ok=True)
        os.makedirs(output_dir_outer, exist_ok=True)
        
        # æŒ‰rankæ’åºå¹¶åªå–å‰100å
        sorted_data = sorted(data, key=lambda x: x.get('rank', float('inf')))
        top_100_data = sorted_data[:100]
        
        logger.info(f"[DataMergerTool] åŸå§‹æ•°æ®{len(data)}æ¡ï¼Œç­›é€‰å‰100ååä¸º{len(top_100_data)}æ¡")
        
        filename = f"merged_data_{timestamp}.csv"
        filepath = os.path.join(output_dir_inner, filename)
        
        # è½¬æ¢ä¸ºDataFrameå¹¶ä¿å­˜
        df = pd.DataFrame(top_100_data)
        
        # å¤„ç†JSONå­—æ®µï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²
        json_columns = ['image_list', 'tag_list', 'brand_list', 'spu_list', 'emotion_dict', 'evaluation_dict']
        for col in json_columns:
            if col in df.columns:
                df[col] = df[col].apply(lambda x: json.dumps(x, ensure_ascii=False) if x else '')
        
        # ç”Ÿæˆå¯¹å¤–è¾“å‡ºçš„ä¸­æ–‡CSVæ–‡ä»¶
        outer_filename = f"basic_data_{timestamp}.csv"
        outer_filepath = os.path.join(output_dir_outer, outer_filename)
        
        # ç­›é€‰column_mappingä¸­å­˜åœ¨çš„åˆ—å¹¶é‡å‘½åä¸ºä¸­æ–‡
        available_columns = [col for col in self.column_mapping.keys() if col in df.columns]
        df_chinese = df[available_columns].copy()
        df_chinese.rename(columns=self.column_mapping, inplace=True)

        # ä¿å­˜CSV
        df.to_csv(filepath, index=False, encoding='utf-8-sig')
        df_chinese.to_csv(outer_filepath, index=False, encoding='utf-8-sig')
        
        logger.info(f"[DataMergerTool] å†…éƒ¨æ•°æ®å·²ä¿å­˜åˆ°: {filepath}")
        logger.info(f"[DataMergerTool] å¯¹å¤–æ•°æ®å·²ä¿å­˜åˆ°: {outer_filepath}")
        return filepath
    
    def _generate_statistics(self, merged_data: List[Dict], keyword: str) -> Dict[str, Any]:
        """ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯"""
        total_count = len(merged_data)
        matched_count = sum(1 for record in merged_data if record['has_note_detail'])
        unmatched_count = total_count - matched_count
        with_brand_count = sum(1 for record in merged_data if record['has_brand_info'])
        
        # è®¡ç®—å”¯ä¸€å“ç‰Œæ•°
        all_brands = set()
        for record in merged_data:
            if record.get('brand_list'):
                try:
                    brands = record['brand_list'] if isinstance(record['brand_list'], list) else json.loads(record['brand_list'] or '[]')
                    all_brands.update(brands)
                except:
                    pass
        
        # è®¡ç®—å¹³å‡åˆå¹¶æ’å
        merged_ranks = [record['merged_rank'] for record in merged_data if record.get('merged_rank') is not None]
        avg_merged_rank = sum(merged_ranks) / len(merged_ranks) if merged_ranks else 0
        
        # è®¡ç®—è´¦æˆ·ç»Ÿè®¡
        account_counts = [record['account_count'] for record in merged_data if record.get('account_count') is not None]
        avg_account_count = sum(account_counts) / len(account_counts) if account_counts else 0
        max_account_count = max(account_counts) if account_counts else 0
        
        return {
            'matched_count': matched_count,
            'unmatched_count': unmatched_count,
            'with_brand_count': with_brand_count,
            'unique_brands': len(all_brands),
            'avg_merged_rank': avg_merged_rank,
            'account_count': max_account_count,  # æ€»å…±æ¶‰åŠçš„è´¦æˆ·æ•°
            'avg_account_per_note': avg_account_count  # å¹³å‡æ¯ä¸ªç¬”è®°å‡ºç°åœ¨å¤šå°‘ä¸ªè´¦æˆ·ä¸­
        } 