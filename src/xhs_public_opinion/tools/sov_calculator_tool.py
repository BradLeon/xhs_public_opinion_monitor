import os
import json
import pandas as pd
import numpy as np
from typing import List, Dict, Any, Optional, Tuple
from crewai.tools import BaseTool
import logging
from datetime import datetime
import glob

# å¯¼å…¥å“ç‰Œæ ‡å‡†åŒ–å·¥å…·
from .brand_normalizer import get_brand_normalizer

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SOVCalculatorTool(BaseTool):
    """SOVè®¡ç®—å·¥å…· - è®¡ç®—å„å“ç‰Œåœ¨å…³é”®è¯ä¸‹çš„å£°é‡å æ¯”ï¼ˆShare of Voiceï¼‰"""
    name: str = "sov_calculator"
    description: str = "åŸºäºåˆå¹¶åçš„CSVæ•°æ®ï¼Œè®¡ç®—å„å“ç‰Œåœ¨æŒ‡å®šå…³é”®è¯ä¸‹çš„SOVï¼ˆå£°é‡å æ¯”ï¼‰"
    
    def _run(self, keyword: str, data_dir: str = "data/export", method: str = "weighted") -> str:
        """
        è®¡ç®—SOV
        
        Args:
            keyword: å…³é”®è¯
            data_dir: æ•°æ®ç›®å½•
            method: è®¡ç®—æ–¹æ³• (simple/weighted/engagement)
                   - simple: åŸºäºç¬”è®°æ•°é‡çš„ç®€å•å æ¯”
                   - weighted: åŸºäºæœç´¢æ’ååŠ æƒçš„å æ¯”
                   - engagement: åŸºäºäº’åŠ¨é‡åŠ æƒçš„å æ¯”
                   
        Returns:
            SOVè®¡ç®—ç»“æœ
        """
        try:
            logger.info(f"[SOVCalculatorTool] å¼€å§‹è®¡ç®—å…³é”®è¯ '{keyword}' çš„SOVï¼Œæ–¹æ³•: {method}")
            
            # 1. æŸ¥æ‰¾å¯¹åº”çš„CSVæ–‡ä»¶
            csv_file = self._find_csv_file(keyword, data_dir)
            if not csv_file:
                return f"æœªæ‰¾åˆ°å…³é”®è¯ '{keyword}' çš„æ•°æ®æ–‡ä»¶ï¼Œè¯·å…ˆä½¿ç”¨DataMergerToolç”Ÿæˆå®½è¡¨æ•°æ®"
            
            # 2. è¯»å–CSVæ•°æ®
            df = pd.read_csv(csv_file)
            logger.info(f"[SOVCalculatorTool] è¯»å–æ•°æ®æ–‡ä»¶: {csv_file}, è®°å½•æ•°: {len(df)}")
            
            # 3. æ•°æ®é¢„å¤„ç†
            processed_df = self._preprocess_data(df)
            
            # 4. æ ¹æ®é€‰æ‹©çš„æ–¹æ³•è®¡ç®—SOV
            if method == "simple":
                sov_results = self._calculate_simple_sov(processed_df)
            elif method == "weighted":
                sov_results = self._calculate_weighted_sov(processed_df)
            elif method == "engagement":
                sov_results = self._calculate_engagement_sov(processed_df)
            else:
                return f"ä¸æ”¯æŒçš„è®¡ç®—æ–¹æ³•: {method}ã€‚æ”¯æŒçš„æ–¹æ³•: simple, weighted, engagement"
            
            # 5. ä¿å­˜ç»“æœ
            result_file = self._save_sov_results(sov_results, keyword, method, data_dir)
            
            # 6. ç”ŸæˆæŠ¥å‘Š
            report = self._generate_sov_report(sov_results, keyword, method, len(processed_df))
            
            return f"""âœ… SOVè®¡ç®—å®Œæˆï¼

ğŸ“Š SOVåˆ†ææŠ¥å‘Š:
{report}

ğŸ“ ç»“æœæ–‡ä»¶: {result_file}
ğŸ“ æ•°æ®æº: {csv_file}"""
            
        except Exception as e:
            logger.error(f"[SOVCalculatorTool] SOVè®¡ç®—å¤±è´¥: {e}")
            return f"SOVè®¡ç®—å¤±è´¥: {str(e)}"
    
    def _find_csv_file(self, keyword: str, data_dir: str) -> Optional[str]:
        """æŸ¥æ‰¾æŒ‡å®šå…³é”®è¯çš„æœ€æ–°CSVæ–‡ä»¶"""
        pattern = os.path.join(data_dir, f"merged_data_{keyword}_*.csv")
        files = glob.glob(pattern)
        
        if not files:
            return None
        
        # è¿”å›æœ€æ–°çš„æ–‡ä»¶
        latest_file = max(files, key=os.path.getctime)
        logger.info(f"[SOVCalculatorTool] æ‰¾åˆ°æ•°æ®æ–‡ä»¶: {latest_file}")
        return latest_file
    
    def _preprocess_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ•°æ®é¢„å¤„ç†"""
        # åªä¿ç•™æœ‰å“ç‰Œä¿¡æ¯çš„è®°å½•
        df_valid = df[df['has_brand_info'] == True].copy()
        
        logger.info(f"[SOVCalculatorTool] æ•°æ®é¢„å¤„ç†: åŸå§‹è®°å½• {len(df)} æ¡ï¼Œæœ‰å“ç‰Œä¿¡æ¯ {len(df_valid)} æ¡")
        
        # å¤„ç†brand_listå­—æ®µ
        expanded_records = []
        
        for _, row in df_valid.iterrows():
            try:
                brand_list_str = row['brand_list']

                if pd.isna(brand_list_str) or brand_list_str == '':
                    continue
                
                # è§£æå“ç‰Œåˆ—è¡¨ - å¤„ç†å¯èƒ½çš„åŒé‡JSONç¼–ç 
                if isinstance(brand_list_str, str):
                    brands = json.loads(brand_list_str)
                    
                    # å¤„ç†åŒé‡ç¼–ç çš„æƒ…å†µ
                    if isinstance(brands, str):
                        try:
                            brands = json.loads(brands)
                            logger.debug(f"åŒé‡è§£ç æˆåŠŸ: {brands}")
                        except (json.JSONDecodeError, TypeError):
                            logger.warning(f"åŒé‡è§£ç å¤±è´¥: {brands}")
                            continue
                            
                else:
                    brands = brand_list_str
                
                if not isinstance(brands, list):
                    continue
                if len(brands) == 0:
                    continue

                logger.info(f"[SOVCalculatorTool] rank: {row['rank']}ï¼Œbrands:  {brands}")
        
                # ä¸ºæ¯ä¸ªå“ç‰Œåˆ›å»ºä¸€æ¡è®°å½•
                for brand in brands:
                    if brand and brand.strip():
                        # æ ‡å‡†åŒ–å“ç‰Œå
                        brand_normalizer = get_brand_normalizer()
                        normalized_brand = brand_normalizer.normalize_brand_name(brand.strip())
                        
                        if normalized_brand:  # åªæœ‰æ ‡å‡†åŒ–åä¸ä¸ºç©ºçš„å“ç‰Œåæ‰å¤„ç†
                            # å°†pandas Seriesè½¬æ¢ä¸ºå­—å…¸ï¼Œç„¶åæ·»åŠ brandå­—æ®µ
                            record = row.to_dict()
                            record['brand'] = normalized_brand
                            record['original_brand'] = brand.strip()  # ä¿ç•™åŸå§‹å“ç‰Œåç”¨äºè°ƒè¯•
                            expanded_records.append(record)
                        
            except (json.JSONDecodeError, TypeError) as e:
                logger.warning(f"è§£æå“ç‰Œåˆ—è¡¨å¤±è´¥: {e}, æ•°æ®: {row['brand_list']}")
                continue
        
        if not expanded_records:
            logger.warning("[SOVCalculatorTool] æ²¡æœ‰æœ‰æ•ˆçš„å“ç‰Œæ•°æ®")
            return pd.DataFrame()
        
        expanded_df = pd.DataFrame(expanded_records)
        
        # å¡«å……ç¼ºå¤±å€¼
        numeric_columns = ['liked_count', 'collected_count', 'comment_count', 'share_count', 'rank']
        for col in numeric_columns:
            if col in expanded_df.columns:
                expanded_df[col] = pd.to_numeric(expanded_df[col], errors='coerce').fillna(0)
        
        logger.info(f"[SOVCalculatorTool] å±•å¼€åçš„å“ç‰Œè®°å½•: {len(expanded_df)} æ¡ï¼Œæ¶‰åŠå“ç‰Œ: {expanded_df['brand'].nunique()} ä¸ª")
        logger.info(f"[SOVCalculatorTool] å±•å¼€åçš„å“ç‰Œè®°å½•: {expanded_df.head(10)}")

        return expanded_df
    
    def _calculate_simple_sov(self, df: pd.DataFrame) -> Dict[str, Any]:
        """è®¡ç®—ç®€å•SOVï¼ˆåŸºäºç¬”è®°æ•°é‡ï¼‰"""

        brand_counts = df['brand'].value_counts()
        total_mentions = len(df)
        
        sov_data = []
        for brand, count in brand_counts.items():
            sov_percentage = (count / total_mentions) * 100
            sov_data.append({
                'brand': brand,
                'mention_count': int(count),
                'sov_percentage': round(sov_percentage, 2),
                'rank': len(sov_data) + 1
            })
        
        return {
            'method': 'simple',
            'total_mentions': total_mentions,
            'unique_brands': len(brand_counts),
            'sov_data': sov_data
        }
    
    def _calculate_weighted_sov(self, df: pd.DataFrame) -> Dict[str, Any]:
        """è®¡ç®—åŠ æƒSOVï¼ˆåŸºäºæœç´¢æ’ååŠ æƒï¼‰"""
        # æ’åæƒé‡ï¼šæ’åè¶Šé å‰æƒé‡è¶Šé«˜
        # æƒé‡å…¬å¼ï¼š1 / rank (æ’åç¬¬1çš„æƒé‡æœ€é«˜)
        df['rank_weight'] = 1 / (df['rank'] + 1)  # +1é¿å…é™¤é›¶
        
        brand_weighted_scores = df.groupby('brand')['rank_weight'].sum()
        total_weight = df['rank_weight'].sum()
        
        sov_data = []
        for brand, weight in brand_weighted_scores.sort_values(ascending=False).items():
            sov_percentage = (weight / total_weight) * 100
            mention_count = len(df[df['brand'] == brand])
            avg_rank = df[df['brand'] == brand]['rank'].mean()
            
            sov_data.append({
                'brand': brand,
                'mention_count': mention_count,
                'weighted_score': round(weight, 4),
                'sov_percentage': round(sov_percentage, 2),
                'avg_rank': round(avg_rank, 2),
                'rank': len(sov_data) + 1
            })
        
        return {
            'method': 'weighted',
            'total_weight': round(total_weight, 4),
            'unique_brands': len(brand_weighted_scores),
            'sov_data': sov_data
        }
    
    def _calculate_engagement_sov(self, df: pd.DataFrame) -> Dict[str, Any]:
        """è®¡ç®—äº’åŠ¨é‡åŠ æƒSOV"""
        # è®¡ç®—æ€»äº’åŠ¨é‡
        df['total_engagement'] = (
            df['liked_count'] + 
            df['collected_count'] + 
            df['comment_count'] + 
            df['share_count']
        )
        
        brand_engagement = df.groupby('brand').agg({
            'total_engagement': 'sum',
            'brand': 'count',  # ç¬”è®°æ•°é‡
            'rank': 'mean'     # å¹³å‡æ’å
        }).rename(columns={'brand': 'mention_count'})
        
        total_engagement = df['total_engagement'].sum()
        
        sov_data = []
        for brand, row in brand_engagement.sort_values('total_engagement', ascending=False).iterrows():
            sov_percentage = (row['total_engagement'] / total_engagement) * 100 if total_engagement > 0 else 0
            
            sov_data.append({
                'brand': brand,
                'mention_count': int(row['mention_count']),
                'total_engagement': int(row['total_engagement']),
                'avg_engagement_per_note': round(row['total_engagement'] / row['mention_count'], 2),
                'sov_percentage': round(sov_percentage, 2),
                'avg_rank': round(row['rank'], 2),
                'rank': len(sov_data) + 1
            })
        
        return {
            'method': 'engagement',
            'total_engagement': int(total_engagement),
            'unique_brands': len(brand_engagement),
            'sov_data': sov_data
        }
    
    def _save_sov_results(self, results: Dict[str, Any], keyword: str, method: str, data_dir: str) -> str:
        """ä¿å­˜SOVè®¡ç®—ç»“æœ"""
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(data_dir, exist_ok=True)
        
        # ç”Ÿæˆæ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"sov_results_{keyword}_{method}_{timestamp}.json"
        filepath = os.path.join(data_dir, filename)
        
        # æ·»åŠ å…ƒæ•°æ®
        results['metadata'] = {
            'keyword': keyword,
            'method': method,
            'calculated_at': datetime.now().isoformat(),
            'tool_version': '1.0'
        }
        
        # ä¿å­˜JSONæ–‡ä»¶
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        # åŒæ—¶ä¿å­˜CSVæ ¼å¼çš„SOVæ•°æ®
        csv_filename = f"sov_results_{keyword}_{method}_{timestamp}.csv"
        csv_filepath = os.path.join(data_dir, csv_filename)
        
        sov_df = pd.DataFrame(results['sov_data'])
        sov_df.to_csv(csv_filepath, index=False, encoding='utf-8-sig')
        
        logger.info(f"[SOVCalculatorTool] SOVç»“æœå·²ä¿å­˜: {filepath}")
        logger.info(f"[SOVCalculatorTool] SOVç»“æœCSV: {csv_filepath}")
        
        return filepath
    
    def _generate_sov_report(self, results: Dict[str, Any], keyword: str, method: str, total_records: int) -> str:
        """ç”ŸæˆSOVæŠ¥å‘Š"""
        sov_data = results['sov_data']
        
        if not sov_data:
            return "æ²¡æœ‰æ‰¾åˆ°å“ç‰ŒSOVæ•°æ®"
        
        report_lines = []
        report_lines.append(f"å…³é”®è¯: {keyword}")
        report_lines.append(f"è®¡ç®—æ–¹æ³•: {method}")
        report_lines.append(f"æ€»è®°å½•æ•°: {total_records}")
        report_lines.append(f"æ¶‰åŠå“ç‰Œæ•°: {results['unique_brands']}")
        report_lines.append("")
        report_lines.append("ğŸ† Top 10 å“ç‰ŒSOVæ’å:")
        report_lines.append("-" * 60)
        
        # æ˜¾ç¤ºå‰10å
        top_brands = sov_data[:10]
        
        if method == 'simple':
            for item in top_brands:
                report_lines.append(
                    f"{item['rank']:2d}. {item['brand']:<20} | "
                    f"ç¬”è®°æ•°: {item['mention_count']:3d} | "
                    f"SOV: {item['sov_percentage']:6.2f}%"
                )
        elif method == 'weighted':
            for item in top_brands:
                report_lines.append(
                    f"{item['rank']:2d}. {item['brand']:<20} | "
                    f"ç¬”è®°æ•°: {item['mention_count']:3d} | "
                    f"å¹³å‡æ’å: {item['avg_rank']:5.2f} | "
                    f"SOV: {item['sov_percentage']:6.2f}%"
                )
        elif method == 'engagement':
            for item in top_brands:
                report_lines.append(
                    f"{item['rank']:2d}. {item['brand']:<20} | "
                    f"ç¬”è®°æ•°: {item['mention_count']:3d} | "
                    f"æ€»äº’åŠ¨: {item['total_engagement']:6d} | "
                    f"SOV: {item['sov_percentage']:6.2f}%"
                )
        
        # æ·»åŠ å¸‚åœºé›†ä¸­åº¦åˆ†æ
        report_lines.append("")
        report_lines.append("ğŸ“ˆ å¸‚åœºé›†ä¸­åº¦åˆ†æ:")
        
        top3_sov = sum(item['sov_percentage'] for item in sov_data[:3])
        top5_sov = sum(item['sov_percentage'] for item in sov_data[:5])
        
        report_lines.append(f"- Top 3 å“ç‰ŒSOVæ€»å’Œ: {top3_sov:.2f}%")
        report_lines.append(f"- Top 5 å“ç‰ŒSOVæ€»å’Œ: {top5_sov:.2f}%")
        
        # å¸‚åœºé›†ä¸­åº¦åˆ¤æ–­
        if top3_sov > 60:
            concentration = "é«˜åº¦é›†ä¸­"
        elif top3_sov > 40:
            concentration = "ä¸­åº¦é›†ä¸­"
        else:
            concentration = "åˆ†æ•£ç«äº‰"
        
        report_lines.append(f"- å¸‚åœºé›†ä¸­åº¦: {concentration}")
        
        return "\n".join(report_lines) 