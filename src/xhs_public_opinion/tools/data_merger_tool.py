import os
import json
import pandas as pd
from typing import List, Dict, Any, Optional
from crewai.tools import BaseTool
from supabase import create_client, Client
import logging
from datetime import datetime
from dotenv import load_dotenv

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DataMergerTool(BaseTool):
    """æ•°æ®æ‹¼æ¥å·¥å…· - å°†æœç´¢ç»“æœè¡¨å’Œç¬”è®°è¯¦æƒ…è¡¨è¿æ¥ç”Ÿæˆå®½è¡¨"""
    name: str = "data_merger"
    description: str = "æ ¹æ®æŒ‡å®šå…³é”®è¯ï¼Œå°†xhs_search_resultå’Œxhs_noteè¡¨æ ¹æ®note_idè¿æ¥ï¼Œç”ŸæˆCSVå®½è¡¨æ–‡ä»¶"
    
    def __init__(self):
        super().__init__()
        self.url = os.getenv("SEO_SUPABASE_URL")
        self.key = os.getenv("SEO_SUPABASE_ANON_KEY")
        
        if not self.url or not self.key:
            raise ValueError("è¯·ç¡®ä¿è®¾ç½®äº† SEO_SUPABASE_URL å’Œ SEO_SUPABASE_ANON_KEY ç¯å¢ƒå˜é‡")
        
        self.client: Client = create_client(self.url, self.key)
    
    def _run(self, keyword: str, output_dir: str = "data/export") -> str:
        """
        æ‰§è¡Œæ•°æ®æ‹¼æ¥ä»»åŠ¡
        
        Args:
            keyword: è¦æŸ¥è¯¢çš„å…³é”®è¯
            output_dir: è¾“å‡ºç›®å½•ï¼Œé»˜è®¤ä¸ºdata/export
            
        Returns:
            è¿”å›æ“ä½œç»“æœä¿¡æ¯
        """
        try:
            logger.info(f"[DataMergerTool] å¼€å§‹ä¸ºå…³é”®è¯ '{keyword}' è¿›è¡Œæ•°æ®æ‹¼æ¥")
            
            # 1. ä»xhs_search_resultè¡¨è·å–æŒ‡å®šå…³é”®è¯çš„æœç´¢ç»“æœ
            search_results = self._get_search_results(keyword)
            if not search_results:
                return f"å…³é”®è¯ '{keyword}' æ²¡æœ‰æ‰¾åˆ°æœç´¢ç»“æœæ•°æ®"
            
            logger.info(f"[DataMergerTool] è·å–åˆ° {len(search_results)} æ¡æœç´¢ç»“æœ")
            
            # 2. æå–æ‰€æœ‰note_id
            note_ids = [result['note_id'] for result in search_results if result.get('note_id')]
            unique_note_ids = list(set(note_ids))  # å»é‡
            
            logger.info(f"[DataMergerTool] æå–åˆ° {len(unique_note_ids)} ä¸ªå”¯ä¸€çš„note_id")
            
            # 3. ä»xhs_noteè¡¨è·å–å¯¹åº”çš„ç¬”è®°è¯¦æƒ…
            note_details = self._get_note_details(unique_note_ids)
            
            logger.info(f"[DataMergerTool] è·å–åˆ° {len(note_details)} æ¡ç¬”è®°è¯¦æƒ…")
            
            # 4. æ•°æ®æ‹¼æ¥
            merged_data = self._merge_data(search_results, note_details)
            
            # 5. ç”ŸæˆCSVæ–‡ä»¶
            csv_path = self._save_to_csv(merged_data, keyword, output_dir)
            
            # 6. ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
            stats = self._generate_statistics(merged_data, keyword)
            
            return f"""âœ… æ•°æ®æ‹¼æ¥å®Œæˆï¼

ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:
- å…³é”®è¯: {keyword}
- æœç´¢ç»“æœè®°å½•: {len(search_results)} æ¡
- å”¯ä¸€ç¬”è®°æ•°: {len(unique_note_ids)} ä¸ª
- æˆåŠŸåŒ¹é…: {stats['matched_count']} æ¡
- æœªåŒ¹é…: {stats['unmatched_count']} æ¡
- è¾“å‡ºæ–‡ä»¶: {csv_path}

ğŸ“ˆ æ•°æ®æ¦‚è§ˆ:
- æ€»è®°å½•æ•°: {len(merged_data)}
- åŒ…å«å“ç‰Œä¿¡æ¯çš„è®°å½•: {stats['with_brand_count']} æ¡
- æ¶‰åŠå“ç‰Œæ•°: {stats['unique_brands']} ä¸ª
- å¹³å‡æœç´¢æ’å: {stats['avg_rank']:.2f}

æ–‡ä»¶å·²ä¿å­˜åˆ°: {csv_path}"""
            
        except Exception as e:
            logger.error(f"[DataMergerTool] æ•°æ®æ‹¼æ¥å¤±è´¥: {e}")
            return f"æ•°æ®æ‹¼æ¥å¤±è´¥: {str(e)}"
    
    def _get_search_results(self, keyword: str) -> List[Dict[str, Any]]:
        """ä»xhs_search_resultè¡¨è·å–æŒ‡å®šå…³é”®è¯çš„æœç´¢ç»“æœ"""
        try:
            response = (
                self.client.table("xhs_search_result")
                .select("*")
                .eq("keyword", keyword)
                .order("rank", desc=False)  # æŒ‰æ’åå‡åºæ’åˆ—
                .execute()
            )
            return response.data
        except Exception as e:
            logger.error(f"è·å–æœç´¢ç»“æœå¤±è´¥: {e}")
            return []
    
    def _get_note_details(self, note_ids: List[str]) -> List[Dict[str, Any]]:
        """ä»xhs_noteè¡¨è·å–æŒ‡å®šnote_idçš„ç¬”è®°è¯¦æƒ…"""
        if not note_ids:
            return []
        
        try:
            # Supabaseçš„inæ“ä½œ
            response = (
                self.client.table("xhs_note")
                .select("*")
                .in_("note_id", note_ids)
                .execute()
            )
            return response.data
        except Exception as e:
            logger.error(f"è·å–ç¬”è®°è¯¦æƒ…å¤±è´¥: {e}")
            return []
    
    def _merge_data(self, search_results: List[Dict], note_details: List[Dict]) -> List[Dict]:
        """æ‹¼æ¥æœç´¢ç»“æœå’Œç¬”è®°è¯¦æƒ…æ•°æ®"""
        # å°†ç¬”è®°è¯¦æƒ…è½¬æ¢ä¸ºå­—å…¸ï¼Œä»¥note_idä¸ºkey
        note_dict = {note['note_id']: note for note in note_details}
        
        merged_data = []
        
        for search_record in search_results:
            note_id = search_record.get('note_id')
            note_detail = note_dict.get(note_id, {})
            
            # åˆå¹¶è®°å½•
            merged_record = {
                # æœç´¢ç»“æœå­—æ®µ
                'search_id': search_record.get('id'),
                'keyword': search_record.get('keyword'),
                'search_account': search_record.get('search_account'),
                'rank': search_record.get('rank'),
                'note_id': note_id,
                
                # ç¬”è®°è¯¦æƒ…å­—æ®µ
                'note_table_id': note_detail.get('id'),
                'title': note_detail.get('title'),
                'type': note_detail.get('type'),
                'desc': note_detail.get('desc'),
                'note_url': note_detail.get('note_url'),
                'video_url': note_detail.get('video_url'),
                'image_list': note_detail.get('image_list'),
                'tag_list': note_detail.get('tag_list'),
                'author_id': note_detail.get('author_id'),
                'nickname': note_detail.get('nickname'),
                'liked_count': note_detail.get('liked_count'),
                'collected_count': note_detail.get('collected_count'),
                'comment_count': note_detail.get('comment_count'),
                'share_count': note_detail.get('share_count'),
                'brand_list': note_detail.get('brand_list'),
                'spu_list': note_detail.get('spu_list'),
                'emotion_dict': note_detail.get('emotion_dict'),
                'evaluation_dict': note_detail.get('evaluation_dict'),
                
                # æ•°æ®çŠ¶æ€æ ‡è¯†
                'has_note_detail': bool(note_detail),
                'has_brand_info': bool(note_detail.get('brand_list')),
            }
            
            merged_data.append(merged_record)
        
        return merged_data
    
    def _save_to_csv(self, data: List[Dict], keyword: str, output_dir: str) -> str:
        """ä¿å­˜æ•°æ®åˆ°CSVæ–‡ä»¶"""
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(output_dir, exist_ok=True)
        
        # ç”Ÿæˆæ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"merged_data_{keyword}_{timestamp}.csv"
        filepath = os.path.join(output_dir, filename)
        
        # è½¬æ¢ä¸ºDataFrameå¹¶ä¿å­˜
        df = pd.DataFrame(data)
        
        # å¤„ç†JSONå­—æ®µï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²
        json_columns = ['image_list', 'tag_list', 'brand_list', 'spu_list', 'emotion_dict', 'evaluation_dict']
        for col in json_columns:
            if col in df.columns:
                df[col] = df[col].apply(lambda x: json.dumps(x, ensure_ascii=False) if x else '')
        
        # ä¿å­˜CSV
        df.to_csv(filepath, index=False, encoding='utf-8-sig')
        
        logger.info(f"[DataMergerTool] æ•°æ®å·²ä¿å­˜åˆ°: {filepath}")
        return filepath
    
    def _generate_statistics(self, merged_data: List[Dict], keyword: str) -> Dict[str, Any]:
        """ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯"""
        total_count = len(merged_data)
        matched_count = sum(1 for record in merged_data if record['has_note_detail'])
        unmatched_count = total_count - matched_count
        with_brand_count = sum(1 for record in merged_data if record['has_brand_info'])
        
        # è®¡ç®—å”¯ä¸€å“ç‰Œæ•°
        all_brands = set()
        for record in merged_data:
            if record.get('brand_list'):
                try:
                    brands = record['brand_list'] if isinstance(record['brand_list'], list) else json.loads(record['brand_list'] or '[]')
                    all_brands.update(brands)
                except:
                    pass
        
        # è®¡ç®—å¹³å‡æ’å
        ranks = [record['rank'] for record in merged_data if record.get('rank') is not None]
        avg_rank = sum(ranks) / len(ranks) if ranks else 0
        
        return {
            'matched_count': matched_count,
            'unmatched_count': unmatched_count,
            'with_brand_count': with_brand_count,
            'unique_brands': len(all_brands),
            'avg_rank': avg_rank
        } 