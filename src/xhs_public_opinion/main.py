#!/usr/bin/env python
import sys
import warnings
import os
import json
import re
from datetime import datetime
from dotenv import load_dotenv
import logging

from xhs_public_opinion.crew import XhsPublicOpinionCrew
from xhs_public_opinion.tools import (
    DatabaseReaderTool,
    DataMergerTool,
    SOVCalculatorTool,
    MultimodalBrandAnalyzer,
    SingleNoteWriterTool,
    BrandSentimentExtractorTool
)

logger = logging.getLogger(__name__)

warnings.filterwarnings("ignore", category=SyntaxWarning, module="pysbd")

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

def _check_environment() -> bool:
    """æ£€æŸ¥å¿…è¦çš„ç¯å¢ƒå˜é‡"""
    required_env_vars = ['SEO_SUPABASE_URL', 'SEO_SUPABASE_ANON_KEY', 'DASHSCOPE_API_KEY']
    missing_vars = [var for var in required_env_vars if not os.getenv(var)]
    
    if missing_vars:
        print(f"é”™è¯¯ï¼šç¼ºå°‘å¿…è¦çš„ç¯å¢ƒå˜é‡: {', '.join(missing_vars)}")
        print("è¯·åˆ›å»º .env æ–‡ä»¶å¹¶è®¾ç½®ä»¥ä¸‹å˜é‡ï¼š")
        for var in missing_vars:
            print(f"{var}=your_{var.lower()}")
        return False
    return True

def run():
    """
    è¿è¡Œå°çº¢ä¹¦å…¬å…±èˆ†æƒ…åˆ†æï¼ˆå¤šæ¨¡æ€ç‰ˆæœ¬ï¼šæ”¯æŒæ–‡æœ¬ã€å›¾ç‰‡å’Œè§†é¢‘å†…å®¹ï¼‰
    æµç¨‹ï¼šæ•°æ®åº“è¯»å– â†’ AIå†…å®¹åˆ†æ â†’ æ•°æ®åº“å†™å…¥
    """
    try:
        '''
        # ç¯å¢ƒæ£€æŸ¥
        if not _check_environment():
            return None
        
        print("ğŸš€ å¼€å§‹æ‰§è¡Œå°çº¢ä¹¦å…¬å…±èˆ†æƒ…åˆ†æï¼ˆå¤šæ¨¡æ€ç‰ˆæœ¬ï¼‰...")
        print(f"ğŸ“… å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("="*60)
        
        # åˆå§‹åŒ–å·¥å…·
        multimodal_analyzer = MultimodalBrandAnalyzer()
        if not multimodal_analyzer.client:
            print("âŒ å¤šæ¨¡æ€åˆ†æå™¨åˆå§‹åŒ–å¤±è´¥ï¼Œè¯·æ£€æŸ¥DASHSCOPE_API_KEY")
            return None
        
        db_reader = DatabaseReaderTool()
        db_writer = SingleNoteWriterTool()
        
        # è¯»å–å¾…å¤„ç†æ•°æ®
        raw_data = db_reader._run(batch_size=100)
        if not raw_data or "æ²¡æœ‰æ‰¾åˆ°æœªå¤„ç†çš„ç¬”è®°æ•°æ®" in raw_data:
            print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°æœªå¤„ç†çš„æ•°æ®")
            return None
        
        # è§£ææ•°æ®
        notes_data = json.loads(raw_data)
        all_notes = notes_data.get('notes', [])
        total_notes = len(all_notes)
        
        print(f"ğŸ“‹ å…±è¯»å– {total_notes} æ¡å¾…å¤„ç†ç¬”è®°")
        print("="*60)
        
        # å¤„ç†æ¯æ¡ç¬”è®°
        success_count = 0
        failed_count = 0
        
        for i, note in enumerate(all_notes, 1):
            try:
                print(f"\nğŸ”„ å¤„ç†ç¬¬ {i}/{total_notes} æ¡ç¬”è®°...")
                
                # ç¡®å®šå†…å®¹ç±»å‹
                content_type = "video" if note.get('type') == 'video' and note.get('video_url') else \
                             "image" if note.get('image_list') else "text"
                
                # åˆ†æå†…å®¹
                result = multimodal_analyzer._run(json.dumps(note, ensure_ascii=False), content_type)
                parsed_result = json.loads(result)
                
                if parsed_result.get('_analysis_failed', False):
                    print(f"   âš ï¸ åˆ†æå¤±è´¥: {parsed_result.get('_error_message', 'æœªçŸ¥é”™è¯¯')}")
                    failed_count += 1
                    continue
                
                # æ·»åŠ ç¬”è®°IDå¹¶å†™å…¥æ•°æ®åº“
                parsed_result['note_id'] = note['note_id']
                write_result = db_writer._run(parsed_result)
                
                if "âœ…" in write_result:
                    success_count += 1
                    print(f"   âœ… å¤„ç†å®Œæˆ")
                else:
                    failed_count += 1
                    print(f"   âŒ å†™å…¥å¤±è´¥: {write_result}")
                    
            except Exception as e:
                failed_count += 1
                print(f"   âŒ å¤„ç†å¼‚å¸¸: {e}")
                logger.error(f"å¤„ç†å¼‚å¸¸: {str(e)}", exc_info=True)
        
        # æ‰“å°ç»Ÿè®¡ç»“æœ
        print("\n" + "="*60)
        print("ğŸ“Š å¤„ç†å®Œæˆç»Ÿè®¡:")
        print(f"âœ… æˆåŠŸ: {success_count} æ¡")
        print(f"âŒ å¤±è´¥: {failed_count} æ¡")
        print(f"ğŸ“ˆ æˆåŠŸç‡: {(success_count/total_notes*100):.1f}%")
        print(f"â° å®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("="*60)
        '''
        success_count = 1

        # åç»­å¤„ç†
        if success_count > 0:
            merged_data_path = _basic_data_merger(keyword="ä¸°ç›ˆè“¬æ¾æ´—å‘æ°´")
            _sov_calculator(keyword="ä¸°ç›ˆè“¬æ¾æ´—å‘æ°´")
            _extract_brand_sentiment(keyword="ä¸°ç›ˆè“¬æ¾æ´—å‘æ°´", brand="Living Proof", csv_input_path=merged_data_path)  # å¯ä»¥æŒ‡å®šç‰¹å®šå“ç‰Œæˆ–ç•™ç©ºæå–æ‰€æœ‰å“ç‰Œ
        
        return True
        
    except Exception as e:
        print(f"âŒ æ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        print("è¯¦ç»†é”™è¯¯ä¿¡æ¯è¯·æŸ¥çœ‹æ—¥å¿—")
        return None

def _basic_data_merger(keyword: str) -> str:
    """æ„é€ åŸºç¡€æ•°æ®é›†"""
    data_merger_tool = DataMergerTool()
    res = data_merger_tool._run(keyword)
    return res


def _sov_calculator(keyword: str) -> bool:
    """è®¡ç®—SOV"""
    print("ğŸ“– è®¡ç®—SOV...")
    sov_calculator_tool = SOVCalculatorTool()
    res = sov_calculator_tool._run(keyword, method="simple")
    return "è®¡ç®—å¤±è´¥" not in res

def _extract_brand_sentiment(keyword: str, brand: str = "", csv_input_path: str = "") -> bool:
    """æå–å“ç‰Œæƒ…æ„Ÿåˆ†æç»“æœ"""
    print(f"ğŸ’ æå–å“ç‰Œæƒ…æ„Ÿåˆ†æ - keyword: {keyword}, brand: {brand or 'æ‰€æœ‰å“ç‰Œ'}...")
    brand_sentiment_extractor = BrandSentimentExtractorTool()
    res = brand_sentiment_extractor._run(keyword=keyword, brand=brand, csv_input_path=csv_input_path)
    
    if "å¤„ç†å¤±è´¥" in res:
        print(f"   âŒ å“ç‰Œæƒ…æ„Ÿåˆ†æå¤±è´¥: {res}")
        return False
    else:
        return True

def train():
    """è®­ç»ƒcrewæ¨¡å‹"""
    if len(sys.argv) < 3:
        print("ä½¿ç”¨æ–¹æ³•: python -m xhs_public_opinion.main train <è¿­ä»£æ¬¡æ•°> <è®­ç»ƒæ–‡ä»¶å>")
        return None
        
    try:
        print(f"ğŸ¯ å¼€å§‹è®­ç»ƒæ¨¡å‹ï¼Œè¿­ä»£æ¬¡æ•°: {sys.argv[1]}")
        result = XhsPublicOpinionCrew().crew().train(
            n_iterations=int(sys.argv[1]), 
            filename=sys.argv[2], 
            inputs={"analysis_type": "å°çº¢ä¹¦ç¬”è®°æƒ…æ„Ÿåˆ†æè®­ç»ƒ"}
        )
        print("âœ… è®­ç»ƒå®Œæˆ!")
        return result

    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        print("è¯¦ç»†é”™è¯¯ä¿¡æ¯è¯·æŸ¥çœ‹æ—¥å¿—")
        return None

def replay():
    """é‡æ”¾crewæ‰§è¡Œè¿‡ç¨‹"""
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•: python -m xhs_public_opinion.main replay <ä»»åŠ¡ID>")
        return None
        
    try:
        print(f"ğŸ”„ å¼€å§‹é‡æ”¾ä»»åŠ¡: {sys.argv[1]}")
        result = XhsPublicOpinionCrew().crew().replay(task_id=sys.argv[1])
        print("âœ… é‡æ”¾å®Œæˆ!")
        return result

    except Exception as e:
        print(f"âŒ é‡æ”¾è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        print("è¯¦ç»†é”™è¯¯ä¿¡æ¯è¯·æŸ¥çœ‹æ—¥å¿—")
        return None

def test():
    """æµ‹è¯•crewåŠŸèƒ½"""
    try:
        print("ğŸ§ª å¼€å§‹æµ‹è¯•crewåŠŸèƒ½...")
        result = XhsPublicOpinionCrew().crew().test(
            n_iterations=1,
            openai_model_name="gpt-4o-mini"
        )
        print("âœ… æµ‹è¯•å®Œæˆ!")
        return result

    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        print("è¯¦ç»†é”™è¯¯ä¿¡æ¯è¯·æŸ¥çœ‹æ—¥å¿—")
        return None

if __name__ == "__main__":
    if len(sys.argv) > 1:
        command = sys.argv[1]
        if command == "train":
            train()
        elif command == "replay": 
            replay()
        elif command == "test":
            test()
        else:
            print("æœªçŸ¥å‘½ä»¤ï¼Œä½¿ç”¨ run() æ‰§è¡Œé»˜è®¤åˆ†æ")
            run()
    else:
        run()
