import os
import json
import pandas as pd
import glob
from typing import Dict, Any, List, Optional
from crewai.tools import BaseTool
from supabase import create_client, Client
import logging
from datetime import datetime
from .brand_normalizer import get_brand_normalizer
from .brand_normalizer import BrandNormalizer

logger = logging.getLogger(__name__)

class BrandSentimentExtractorTool(BaseTool):
    """å“ç‰Œæƒ…æ„Ÿæå–åŠ©æ‰‹ - åŸºäºkeywordå’Œbrandä»DataMergerToolçš„CSVè¾“å‡ºä¸­æå–æƒ…æ„Ÿå€¾å‘å’Œé«˜é¢‘è¯"""
    name: str = "brand_sentiment_extractor"
    description: str = "åŸºäºæŒ‡å®šçš„keywordå’Œbrandï¼Œä»DataMergerToolç”Ÿæˆçš„CSVæ–‡ä»¶ä¸­æå–å“ç‰Œæƒ…æ„Ÿå€¾å‘å’Œé«˜é¢‘è¯ï¼Œè¾“å‡ºåˆ°CSVæ–‡ä»¶å¹¶å†™å…¥æ•°æ®åº“"
    brand_normalizer: Optional[BrandNormalizer] = None
    column_mapping: Dict[str, str] = {}
    
    # å£°æ˜Pydanticå­—æ®µ
    url: Optional[str] = None
    key: Optional[str] = None
    client: Optional[Client] = None

    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        # åˆå§‹åŒ–å“ç‰Œæ ‡å‡†åŒ–å™¨
        self.brand_normalizer = get_brand_normalizer()
        
        # åˆå§‹åŒ–Supabaseæ•°æ®åº“è¿æ¥
        self.url = os.getenv("SEO_SUPABASE_URL")
        self.key = os.getenv("SEO_SUPABASE_ANON_KEY")
        
        if self.url and self.key:
            self.client = create_client(self.url, self.key)
        else:
            logger.warning("Supabaseç¯å¢ƒå˜é‡æœªè®¾ç½®ï¼Œå°†è·³è¿‡æ•°æ®åº“å†™å…¥")
            self.client = None
        
        self.column_mapping = {
        "keyword": "æœç´¢å…³é”®è¯",
        "brand": "ç›‘æµ‹å“ç‰Œ",
        "rank": "æœç´¢æ’å",
        "note_id": "ç¬”è®°ID",
        "type": "ç¬”è®°ç±»å‹",
        "title": "ç¬”è®°æ ‡é¢˜",
        "desc": "ç¬”è®°æè¿°",
        "note_url": "ç¬”è®°URL",
        "author_id": "ä½œè€…ID",
        "nickname": "ä½œè€…æ˜µç§°",
        "last_update_time": "ç¬”è®°æœ€åæ›´æ–°æ—¶é—´",
        "liked_count": "ç‚¹èµæ•°",
        "collected_count": "æ”¶è—æ•°",
        "comment_count": "è¯„è®ºæ•°",
        "share_count": "åˆ†äº«æ•°",
        "brand_emotion": "å“ç‰Œæƒ…æ„Ÿå€¾å‘", 
        "brand_keywords": "å“ç‰Œé«˜é¢‘è¯",
        "data_crawler_time": "æ•°æ®é‡‡é›†æ—¶é—´",
    }
    
    def _run(self, keyword: str, brand: str = "", output_filename: str = "", csv_input_path: str = "") -> str:
        """
        æå–æŒ‡å®škeywordå’Œbrandçš„ç¬”è®°è¯¦æƒ…
        
        Args:
            keyword: æœç´¢å…³é”®è¯
            brand: å“ç‰Œåç§°ï¼ˆå¯é€‰ï¼Œå¦‚æœä¸ºç©ºåˆ™æå–æ‰€æœ‰å“ç‰Œï¼‰
            output_filename: è¾“å‡ºæ–‡ä»¶åï¼ˆå¯é€‰ï¼‰
            csv_input_path: æŒ‡å®šCSVè¾“å…¥æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œé»˜è®¤è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°æ–‡ä»¶ï¼‰
        
        Returns:
            str: è¾“å‡ºæ–‡ä»¶è·¯å¾„æˆ–é”™è¯¯ä¿¡æ¯
        """
        try:
            logger.info(f"[BrandSentimentExtractor] å¼€å§‹æå–å“ç‰Œæƒ…æ„Ÿåˆ†ææ•°æ® - å…³é”®è¯: {keyword}, å“ç‰Œ: {brand or 'æ‰€æœ‰å“ç‰Œ'}")
            
            # 1. è¯»å–CSVæ•°æ®
            df = self._load_csv_data(csv_input_path)
            if df is None or df.empty:
                return f"âŒ æœªæ‰¾åˆ°å…³é”®è¯ '{keyword}' çš„æ•°æ®æ–‡ä»¶"
            
            logger.info(f"[BrandSentimentExtractor] è¯»å–åˆ° {len(df)} æ¡åŸå§‹è®°å½•")
            
            # 2. ç­›é€‰æœ‰å“ç‰Œä¿¡æ¯çš„è®°å½•
            df_with_brands = df[df['has_brand_info'] == True].copy()
            logger.info(f"[BrandSentimentExtractor] ç­›é€‰å‡º {len(df_with_brands)} æ¡åŒ…å«å“ç‰Œä¿¡æ¯çš„è®°å½•")
            
            if df_with_brands.empty:
                return f"âŒ å…³é”®è¯ '{keyword}' çš„æ•°æ®ä¸­æ²¡æœ‰åŒ…å«å“ç‰Œä¿¡æ¯çš„è®°å½•"
            
            # 3. å“ç‰Œç­›é€‰ï¼ˆå¦‚æœæŒ‡å®šäº†å“ç‰Œï¼‰
            if brand:
                # æ ‡å‡†åŒ–è¾“å…¥çš„å“ç‰Œå
                normalized_target_brand = self.brand_normalizer.normalize_brand_name(brand)
                logger.info(f"[BrandSentimentExtractor] æ ‡å‡†åŒ–ç›®æ ‡å“ç‰Œ: '{brand}' -> '{normalized_target_brand}'")
                
                df_filtered = self._filter_by_brand(df_with_brands, normalized_target_brand)
                if df_filtered.empty:
                    return f"âŒ æœªæ‰¾åˆ°å“ç‰Œ '{brand}' (æ ‡å‡†åŒ–ä¸º: '{normalized_target_brand}') çš„ç›¸å…³è®°å½•"
                
                logger.info(f"[BrandSentimentExtractor] ç­›é€‰å‡ºå“ç‰Œ '{normalized_target_brand}' çš„ {len(df_filtered)} æ¡è®°å½•")
            else:
                df_filtered = df_with_brands
                normalized_target_brand = ""
            
            # 4. å¤„ç†æ•°æ®
            processed_data = self._process_data(df_filtered, normalized_target_brand)
            
            # 5. ç”ŸæˆCSVè¾“å‡º
            output_path = self._generate_csv_output(processed_data, keyword, normalized_target_brand, output_filename)
            
            # 6. å†™å…¥æ•°æ®åº“
            db_write_result = self._write_to_database(processed_data, keyword, normalized_target_brand)
            
            logger.info(f"âœ… [BrandSentimentExtractor] å“ç‰Œæƒ…æ„Ÿåˆ†æå®Œæˆ!")
            logger.info(f"ğŸ“Š å¤„ç†ç»Ÿè®¡:")
            logger.info(f"   - æ€»è®°å½•æ•°: {len(processed_data)}")
            logger.info(f"   - CSVè¾“å‡ºæ–‡ä»¶: {output_path}")
            logger.info(f"   - æ•°æ®åº“å†™å…¥: {db_write_result}")
            
            return output_path
            
        except Exception as e:
            error_msg = f"[BrandSentimentExtractor] å¤„ç†å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            return error_msg
    
    def _load_csv_data(self, csv_input_path: str = "") -> pd.DataFrame:
        """è¯»å–DataMergerToolç”Ÿæˆçš„CSVæ–‡ä»¶"""
        try:
            if csv_input_path and os.path.exists(csv_input_path):
                # ä½¿ç”¨æŒ‡å®šçš„CSVæ–‡ä»¶è·¯å¾„
                logger.info(f"[BrandSentimentExtractor] ä½¿ç”¨æŒ‡å®šçš„CSVæ–‡ä»¶: {csv_input_path}")
                return pd.read_csv(csv_input_path, encoding='utf-8-sig')
            
        except Exception as e:
            logger.error(f"[BrandSentimentExtractor] è¯»å–CSVæ–‡ä»¶å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def _filter_by_brand(self, df: pd.DataFrame, target_brand: str) -> pd.DataFrame:
        """æ ¹æ®å“ç‰Œåç­›é€‰è®°å½•ï¼ˆä½¿ç”¨æ ‡å‡†åŒ–åŒ¹é…ï¼‰"""
        if not target_brand:
            return df
        
        def contains_target_brand(brand_list_str):
            """æ£€æŸ¥å“ç‰Œåˆ—è¡¨ä¸­æ˜¯å¦åŒ…å«ç›®æ ‡å“ç‰Œ"""
            if not brand_list_str:
                return False
            
            try:
                # è§£æå“ç‰Œåˆ—è¡¨
                if isinstance(brand_list_str, str):
                    brand_list = json.loads(brand_list_str)
                else:
                    brand_list = brand_list_str
                
                if not isinstance(brand_list, list):
                    return False
                
                # æ£€æŸ¥æ˜¯å¦æœ‰åŒ¹é…çš„å“ç‰Œï¼ˆå·²ç»æ˜¯æ ‡å‡†åŒ–åçš„åç§°ï¼‰
                for brand in brand_list:
                    if isinstance(brand, str) and brand.strip() == target_brand:
                        return True
                
                return False
                
            except (json.JSONDecodeError, TypeError, AttributeError):
                return False
        
        # åº”ç”¨ç­›é€‰æ¡ä»¶
        mask = df['brand_list'].apply(contains_target_brand)
        return df[mask].copy()
    
    def _parse_json_field(self, field_value: Any) -> Any:
        """å®‰å…¨è§£æJSONå­—æ®µ"""
        # é¦–å…ˆæ£€æŸ¥æ˜¯å¦ä¸ºpandasçš„NaNå€¼
        if pd.isna(field_value):
            return []
        
        if isinstance(field_value, str):
            # å¤„ç†ç©ºå­—ç¬¦ä¸²
            if not field_value.strip():
                return []
            try:
                return json.loads(field_value)
            except (json.JSONDecodeError, TypeError):
                return field_value
        
        # å¦‚æœå·²ç»æ˜¯åˆ—è¡¨æˆ–å­—å…¸ï¼Œç›´æ¥è¿”å›
        if isinstance(field_value, (list, dict)):
            return field_value
        
        # å…¶ä»–æƒ…å†µè¿”å›ç©ºåˆ—è¡¨
        return []
    
    def _get_safe_field_value(self, row: pd.Series, field_name: str, default_value: str = "") -> str:
        """å®‰å…¨è·å–å­—æ®µå€¼ï¼Œå¤„ç†NaNæƒ…å†µ"""
        value = row.get(field_name, default_value)
        # æ£€æŸ¥æ˜¯å¦ä¸ºNaNï¼ˆpandasä¸­çš„ç©ºå€¼æ˜¯floatç±»å‹ï¼‰
        if pd.isna(value):
            return default_value
        return str(value)

    def _process_data(self, df: pd.DataFrame, target_brand: str) -> List[Dict]:
        """å¤„ç†CSVæ•°æ®ï¼Œæå–å…³é”®ä¿¡æ¯"""
        processed_notes = []
        
        for _, row in df.iterrows():
            try:
                # åŸºæœ¬ä¿¡æ¯ - ä½¿ç”¨å®‰å…¨è·å–æ–¹æ³•
                note_info = {
                    "note_id": self._get_safe_field_value(row, "note_id"),
                    "title": self._get_safe_field_value(row, "title"),
                    "desc": self._get_safe_field_value(row, "desc"),
                    "type": self._get_safe_field_value(row, "type"),
                    "author_id": self._get_safe_field_value(row, "author_id"),
                    "nickname": self._get_safe_field_value(row, "nickname"),
                    "note_url": self._get_safe_field_value(row, "note_url"),
                    "video_url": self._get_safe_field_value(row, "video_url"),
                    
                    # æœç´¢ç›¸å…³ä¿¡æ¯
                    "keyword": self._get_safe_field_value(row, "keyword"),
                    "rank": self._get_safe_field_value(row, "rank"),
                    "search_account": self._get_safe_field_value(row, "search_account"),
                    "last_update_time": self._get_safe_field_value(row, "last_update_time"),
                    "data_crawler_time": self._get_safe_field_value(row, "data_crawler_time"),
                    
                    # äº’åŠ¨æ•°æ®
                    "liked_count": self._get_safe_field_value(row, "liked_count"),
                    "collected_count": self._get_safe_field_value(row, "collected_count"),
                    "comment_count": self._get_safe_field_value(row, "comment_count"),
                    "share_count": self._get_safe_field_value(row, "share_count"),
                }
                
                # å¤„ç†å“ç‰Œåˆ—è¡¨ - ä½¿ç”¨å®‰å…¨è·å–
                brand_list_raw = self._get_safe_field_value(row, "brand_list", "[]")
                brand_list = self._parse_json_field(brand_list_raw)
                #logger.info(f"[BrandSentimentExtractor] å“ç‰Œåˆ—è¡¨åŸå§‹å€¼: '{brand_list_raw}', è§£æå: {brand_list}")
                
                if isinstance(brand_list, list):
                    note_info["all_brands"] = ", ".join(str(brand) for brand in brand_list if brand)
                else:
                    note_info["all_brands"] = ""
                
                # å¤„ç†SPUåˆ—è¡¨ - ä½¿ç”¨å®‰å…¨è·å–
                spu_list_raw = self._get_safe_field_value(row, "spu_list", "[]")
                spu_list = self._parse_json_field(spu_list_raw)
                #logger.info(f"[BrandSentimentExtractor] SPUåˆ—è¡¨åŸå§‹å€¼: '{spu_list_raw}', è§£æå: {spu_list}")
                
                if isinstance(spu_list, list):
                    note_info["all_spus"] = ", ".join(str(spu) for spu in spu_list if spu)
                else:
                    note_info["all_spus"] = ""
                
                # å¤„ç†æ ‡ç­¾åˆ—è¡¨ - ä½¿ç”¨å®‰å…¨è·å–
                tag_list_raw = self._get_safe_field_value(row, "tag_list", "[]")
                tag_list = self._parse_json_field(tag_list_raw)
                logger.info(f"[BrandSentimentExtractor] æ ‡ç­¾åˆ—è¡¨åŸå§‹å€¼: '{tag_list_raw}', è§£æå: {tag_list}")
                
                if isinstance(tag_list, list):
                    note_info["all_tags"] = ", ".join(str(tag) for tag in tag_list if tag)
                else:
                    note_info["all_tags"] = ""
                
                # å¤„ç†æƒ…æ„Ÿå­—å…¸ - ä½¿ç”¨å®‰å…¨è·å–
                emotion_dict_raw = self._get_safe_field_value(row, "emotion_dict", "{}")
                emotion_dict = self._parse_json_field(emotion_dict_raw)
                #logger.info(f"[BrandSentimentExtractor] æƒ…æ„Ÿå­—å…¸åŸå§‹å€¼: '{emotion_dict_raw}', è§£æå: {emotion_dict}")
                
                # ç¡®ä¿brand_listæ˜¯åˆ—è¡¨ç±»å‹
                safe_brand_list = brand_list if isinstance(brand_list, list) else []
                brand_emotion = self._extract_brand_emotion(emotion_dict, target_brand, safe_brand_list)
                note_info.update(brand_emotion)
                
                # å¤„ç†è¯„ä»·å­—å…¸ - ä½¿ç”¨å®‰å…¨è·å–
                evaluation_dict_raw = self._get_safe_field_value(row, "evaluation_dict", "{}")
                evaluation_dict = self._parse_json_field(evaluation_dict_raw)
                #logger.info(f"[BrandSentimentExtractor] è¯„ä»·å­—å…¸åŸå§‹å€¼: '{evaluation_dict_raw}', è§£æå: {evaluation_dict}")
                
                brand_evaluation = self._extract_brand_evaluation(evaluation_dict, target_brand, safe_brand_list)
                note_info.update(brand_evaluation)
                
                processed_notes.append(note_info)
                
            except Exception as e:
                logger.warning(f"[BrandSentimentExtractor] å¤„ç†ç¬”è®° {self._get_safe_field_value(row, 'note_id', 'unknown')} æ—¶å‡ºé”™: {e}")
                logger.error(f"[BrandSentimentExtractor] è¯¦ç»†é”™è¯¯ä¿¡æ¯: {str(e)}", exc_info=True)
                continue
        
        return processed_notes
    
    def _extract_brand_emotion(self, emotion_dict: Dict, target_brand: str, all_brands: List[str]) -> Dict:
        """æå–æŒ‡å®šå“ç‰Œçš„æƒ…æ„Ÿå€¾å‘"""
        result = {
            "brand": target_brand,
            "brand_emotion": "",
        }
        
        if not emotion_dict:
            return result
        
        # å¦‚æœæŒ‡å®šäº†ç›®æ ‡å“ç‰Œï¼Œä¼˜å…ˆæå–ç›®æ ‡å“ç‰Œçš„æƒ…æ„Ÿ
        if target_brand:
            for brand_name, emotion_info in emotion_dict.items():
                if target_brand.lower() in brand_name.lower():
                    if isinstance(emotion_info, dict):
                        result["brand_emotion"] = emotion_info.get("emotion", "")
                    else:
                        result["brand_emotion"] = str(emotion_info)
                    break
        
        
        return result
    
    def _extract_brand_evaluation(self, evaluation_dict: Dict, target_brand: str, all_brands: List[str]) -> Dict:
        """æå–æŒ‡å®šå“ç‰Œçš„è¯„ä»·å…³é”®è¯"""
        result = {
            "brand_keywords": "",
        }
        
        if not evaluation_dict:
            return result
        
        # å¦‚æœæŒ‡å®šäº†ç›®æ ‡å“ç‰Œï¼Œä¼˜å…ˆæå–ç›®æ ‡å“ç‰Œçš„å…³é”®è¯
        if target_brand:
            for brand_name, keywords in evaluation_dict.items():
                if target_brand.lower() in brand_name.lower():
                    if isinstance(keywords, list):
                        result["brand_keywords"] = ", ".join(keywords)
                    elif isinstance(keywords, dict):
                        # å¦‚æœæ˜¯å­—å…¸æ ¼å¼ï¼Œæå–å…³é”®è¯å’Œé¢‘æ¬¡
                        keyword_list = [f"{k}({v})" for k, v in keywords.items()]
                        result["brand_keywords"] = ", ".join(keyword_list)
                    else:
                        result["brand_keywords"] = str(keywords)
                    break
        
        
        return result
    
    def _generate_csv_output(self, processed_data: List[Dict], keyword: str, brand: str, output_filename: str) -> str:
        """ç”ŸæˆCSVè¾“å‡ºæ–‡ä»¶"""
        # ç”Ÿæˆæ–‡ä»¶å
        if not output_filename:
            timestamp = datetime.now().strftime("%Y%m%d")
            brand_suffix = f"{brand}" if brand else ""
            output_filename = f"å“ç‰Œèˆ†æƒ…_{brand_suffix}_{timestamp}.csv"
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_dir = "outputs" + '/' + keyword
        os.makedirs(output_dir, exist_ok=True)
        output_path = os.path.join(output_dir, output_filename)
        
        # åˆ›å»ºDataFrame
        df = pd.DataFrame(processed_data)
        
        # é‡æ–°æ’åˆ—åˆ—çš„é¡ºåºï¼ŒæŠŠé‡è¦ä¿¡æ¯æ”¾åœ¨å‰é¢
        column_order = [
            "keyword", "rank", "search_account",
            "note_id", "note_url", "title", "desc", "type", 
            "author_id", "nickname", 
            "liked_count", "collected_count", "comment_count", "share_count",
            "brand", "brand_emotion", "brand_keywords",
            "all_brand_emotions", "all_brand_keywords",
        ]

        # ç­›é€‰column_mappingä¸­å­˜åœ¨çš„åˆ—å¹¶é‡å‘½åä¸ºä¸­æ–‡
        available_columns = [col for col in self.column_mapping.keys() if col in df.columns]
        df_copy = df[available_columns].copy()
        df_copy.rename(columns=self.column_mapping, inplace=True)
        
        # è¾“å‡ºåˆ°CSV
        df_copy.to_csv(output_path, index=False, encoding='utf-8-sig')
        
        logger.info(f"[BrandSentimentExtractor] CSVæ–‡ä»¶å·²ä¿å­˜: {output_path}")
        return output_path

    def _write_to_database(self, processed_data: List[Dict], keyword: str, brand: str) -> str:
        """å°†æ•°æ®å†™å…¥Supabaseæ•°æ®åº“è¡¨xhs_keyword_brand_rank_sentiment_result"""
        if not self.client:
            return "âŒ æ•°æ®åº“è¿æ¥æœªåˆå§‹åŒ–ï¼Œè·³è¿‡æ•°æ®åº“å†™å…¥"
        
        if not processed_data:
            return "âŒ æ²¡æœ‰æ•°æ®éœ€è¦å†™å…¥æ•°æ®åº“"
        
        try:
            logger.info(f"[BrandSentimentExtractor] å¼€å§‹å†™å…¥ {len(processed_data)} æ¡è®°å½•åˆ°æ•°æ®åº“...")
            
            # å‡†å¤‡è¦æ’å…¥çš„æ•°æ®
            data_to_insert = []
            
            for note in processed_data:
                # å¤„ç†å¯èƒ½ä¸ºç©ºæˆ–NaNçš„æ•°å€¼å­—æ®µ
                def safe_int(value, default=0):
                    try:
                        if pd.isna(value) or value == '' or value is None:
                            return default
                        return int(float(value))
                    except (ValueError, TypeError):
                        return default
                
                def safe_str(value, default=""):
                    try:
                        if pd.isna(value) or value is None:
                            return default
                        return str(value)
                    except:
                        return default
                
                record = {
                    "keyword": safe_str(note.get("keyword", keyword)),
                    "brand": safe_str(brand) if brand else safe_str(note.get("brand", "")),
                    "rank": safe_int(note.get("rank")),
                    "note_id": safe_str(note.get("note_id")),
                    "type": safe_str(note.get("type")),
                    "title": safe_str(note.get("title")),
                    "desc": safe_str(note.get("desc")),
                    "note_url": safe_str(note.get("note_url")),
                    "author_id": safe_str(note.get("author_id")),
                    "nickname": safe_str(note.get("nickname")),
                    "last_update_time": safe_str(note.get("last_update_time")),
                    "liked_count": safe_int(note.get("liked_count")),
                    "collected_count": safe_int(note.get("collected_count")),
                    "comment_count": safe_int(note.get("comment_count")),
                    "share_count": safe_int(note.get("share_count")),
                    "brand_emotion": safe_str(note.get("brand_emotion")),
                    "brand_keywords": safe_str(note.get("brand_keywords")),
                    "data_crawler_time": safe_str(note.get("data_crawler_time"))
                }
                data_to_insert.append(record)
            
            # æ‰¹é‡æ’å…¥æ•°æ®åˆ°ç›®æ ‡è¡¨
            response = (
                self.client.table("xhs_keyword_brand_rank_sentiment_result")
                .insert(data_to_insert)
                .execute()
            )
            
            if response.data:
                success_count = len(response.data)
                logger.info(f"[BrandSentimentExtractor] âœ… æˆåŠŸå†™å…¥ {success_count} æ¡è®°å½•åˆ°æ•°æ®åº“")
                return f"âœ… æˆåŠŸå†™å…¥ {success_count} æ¡è®°å½•åˆ°æ•°æ®åº“"
            else:
                logger.warning(f"[BrandSentimentExtractor] âš ï¸ æ•°æ®åº“å†™å…¥å®Œæˆï¼Œä½†æœªè¿”å›æ’å…¥è®°å½•æ•°")
                return f"âœ… æ•°æ®åº“å†™å…¥å®Œæˆï¼ˆ{len(data_to_insert)} æ¡è®°å½•ï¼‰"
            
        except Exception as e:
            error_msg = f"å†™å…¥æ•°æ®åº“å¤±è´¥: {str(e)}"
            logger.error(f"[BrandSentimentExtractor] âŒ {error_msg}")
            return f"âŒ {error_msg}" 